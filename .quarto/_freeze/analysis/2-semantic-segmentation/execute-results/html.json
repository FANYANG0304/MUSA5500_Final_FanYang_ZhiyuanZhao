{
  "hash": "43c8beb6c424ebfcbeabe0631093fd34",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Semantic Segmentation\"\nsubtitle: \"Extracting Urban Visual Features from Street View Imagery\"\n---\n\n## Introduction\n\nStreet view images contain rich information about the urban environment, but raw pixel values are not directly useful for analysis. **Semantic segmentation** transforms these images into meaningful categories—identifying what portion of the scene consists of sky, vegetation, buildings, roads, and other urban elements.\n\nThis section documents our approach to extracting quantitative visual features from Google Street View panoramas using deep learning.\n\n## Model Selection\n\n### Why SegFormer?\n\nWe selected **SegFormer-B0** (Xie et al., 2021) for several reasons:\n\n1. **Pre-trained on ADE20K**: This dataset contains 150 semantic classes with excellent coverage of urban scenes\n2. **Efficient architecture**: The B0 variant balances accuracy with computational efficiency, critical for processing ~30,000 images\n3. **Easy deployment**: Available through Hugging Face Transformers, requiring minimal setup\n\n::: {#81d4a90b .cell execution_count=1}\n``` {.python .cell-code}\nfrom transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\nimport torch\n\n# Load pre-trained model\nmodel_name = \"nvidia/segformer-b0-finetuned-ade-512-512\"\nprocessor = SegformerImageProcessor.from_pretrained(model_name)\nmodel = SegformerForSemanticSegmentation.from_pretrained(model_name)\n\n# Check available device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\nmodel.eval()\n```\n:::\n\n\n### Analysis\n\nThe choice of SegFormer over alternatives like DeepLab or PSPNet reflects a pragmatic trade-off. While newer models may achieve marginally higher accuracy on benchmarks, SegFormer's transformer-based architecture provides:\n\n- **Faster inference** (~2 images/second on a mid-range GPU)\n- **Lower memory footprint** (~4GB VRAM at batch size 8)\n- **Robust performance** across diverse urban scenes\n\nFor our application, where we process tens of thousands of images, these practical considerations outweigh small accuracy differences.\n\n---\n\n## Urban Feature Categories\n\n### Category Definition\n\nThe ADE20K dataset provides 150 fine-grained classes, but for urban analysis, we aggregate these into **6 meaningful categories**:\n\n::: {#1dc7ca32 .cell execution_count=2}\n``` {.python .cell-code}\nclass UrbanFeatureExtractor:\n    \"\"\"\n    Extract urban environmental features from semantic segmentation results.\n    \n    Aggregates ADE20K's 150 classes into 6 urban-relevant categories.\n    \"\"\"\n    \n    def __init__(self):\n        # ADE20K class indices for each urban category\n        self.categories = {\n            'sky': [2],                          # sky\n            'vegetation': [4, 9, 17, 66, 72],    # tree, grass, plant, palm, flower\n            'building': [1, 25, 48, 84],         # building, house, skyscraper, booth\n            'road': [6, 11, 52],                 # road, sidewalk, path\n            'vehicle': [20, 80, 83, 102, 127],   # car, bus, truck, van, bicycle\n            'person': [12]                       # person\n        }\n        \n        # Colors for visualization (RGB)\n        self.colors = {\n            'sky': [135, 206, 235],      # Light blue\n            'vegetation': [34, 139, 34],  # Forest green\n            'building': [128, 128, 128],  # Gray\n            'road': [64, 64, 64],         # Dark gray\n            'vehicle': [255, 0, 0],       # Red\n            'person': [255, 192, 203]     # Pink\n        }\n    \n    def calculate_ratios(self, segmentation_map):\n        \"\"\"\n        Calculate the proportion of each category in the image.\n        \n        Parameters:\n        -----------\n        segmentation_map : numpy array\n            2D array of class predictions (H x W)\n        \n        Returns:\n        --------\n        dict : Category ratios (values sum to less than 1.0 due to uncategorized pixels)\n        \"\"\"\n        total_pixels = segmentation_map.size\n        ratios = {}\n        \n        for category, class_ids in self.categories.items():\n            # Count pixels belonging to this category\n            mask = np.isin(segmentation_map, class_ids)\n            pixel_count = np.sum(mask)\n            ratios[f'{category}_ratio'] = pixel_count / total_pixels\n        \n        # Green View Index is a common metric\n        ratios['green_view_index'] = ratios.pop('vegetation_ratio')\n        \n        return ratios\n```\n:::\n\n\n### Analysis\n\nThe choice of category groupings reflects both practical and theoretical considerations:\n\n**Sky** is kept as a single class because it represents a unified visual experience—the sense of \"openness\" that comes from seeing sky rather than obstructed views.\n\n**Vegetation** combines multiple green elements (trees, grass, plants) because prior research on **Green View Index** (GVI) treats all vegetation similarly for psychological impact (Asgarzadeh et al., 2012).\n\n**Buildings** includes multiple structure types because our interest is in the overall built environment density, not architectural distinctions.\n\n**Road** includes sidewalks and paths because they collectively represent the \"transportation infrastructure\" visible in the scene.\n\nThe remaining pixels belong to ADE20K classes we don't explicitly categorize (furniture, signage, etc.). These typically constitute 5-15% of the image.\n\n---\n\n## Processing Pipeline\n\n### Single Image Processing\n\n::: {#808f6b93 .cell execution_count=3}\n``` {.python .cell-code}\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\ndef segment_image(image_path, model, processor, device):\n    \"\"\"\n    Perform semantic segmentation on a single street view image.\n    \n    Returns both the segmentation map and calculated feature ratios.\n    \"\"\"\n    # Load and prepare image\n    image = cv2.imread(str(image_path))\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    pil_image = Image.fromarray(image_rgb)\n    \n    # Process through model\n    inputs = processor(images=pil_image, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n    \n    # Upsample to original resolution\n    h, w = image.shape[:2]\n    upsampled = torch.nn.functional.interpolate(\n        logits,\n        size=(h, w),\n        mode='bilinear',\n        align_corners=False\n    )\n    \n    # Get class predictions\n    segmentation = upsampled.argmax(dim=1).squeeze().cpu().numpy()\n    \n    return segmentation, image\n```\n:::\n\n\n### Analysis\n\nThe segmentation process involves several important technical details:\n\n**Resolution handling**: SegFormer processes images at 512×512 pixels internally. Since our panoramas are 3328×1664, we need to **upsample** the predictions back to the original resolution. Bilinear interpolation preserves smooth boundaries between classes.\n\n**Batch vs. single processing**: For the happiness points (n=28), we process images individually to also generate visualizations for quality checking. For the full dataset (~30,000 images), we use batch processing for efficiency.\n\n---\n\n### Batch Processing for Full Dataset\n\n::: {#ff79aaf0 .cell execution_count=4}\n``` {.python .cell-code}\nfrom torch.utils.data import Dataset, DataLoader\n\nclass GSVDataset(Dataset):\n    \"\"\"Custom dataset for batch processing street view images.\"\"\"\n    \n    def __init__(self, image_list, processor):\n        self.image_list = image_list\n        self.processor = processor\n    \n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self, idx):\n        item = self.image_list[idx]\n        image = cv2.imread(str(item['path']))\n        \n        if image is None:\n            return {'valid': False, 'pano_id': item['pano_id']}\n        \n        h, w = image.shape[:2]\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        inputs = self.processor(images=Image.fromarray(image_rgb), return_tensors=\"pt\")\n        \n        return {\n            'pixel_values': inputs['pixel_values'].squeeze(0),\n            'pano_id': item['pano_id'],\n            'point_id': item['point_id'],\n            'height': h,\n            'width': w,\n            'valid': True\n        }\n\n\ndef process_batch(batch, model, device, feature_extractor):\n    \"\"\"Process a batch of images through the segmentation model.\"\"\"\n    pixel_values = batch['pixel_values'].to(device)\n    \n    with torch.no_grad():\n        outputs = model(pixel_values=pixel_values)\n        predictions = outputs.logits.argmax(dim=1).cpu().numpy()\n    \n    results = []\n    for i in range(len(predictions)):\n        # Resize prediction to original dimensions\n        pred_resized = cv2.resize(\n            predictions[i].astype(np.uint8),\n            (batch['width'][i], batch['height'][i]),\n            interpolation=cv2.INTER_NEAREST\n        )\n        \n        # Calculate ratios\n        ratios = feature_extractor.calculate_ratios(pred_resized)\n        ratios['pano_id'] = batch['pano_id'][i]\n        ratios['point_id'] = batch['point_id'][i]\n        results.append(ratios)\n    \n    return results\n```\n:::\n\n\n### Analysis\n\nBatch processing reduces total processing time from ~100+ hours to approximately **4 hours** on a single GPU. Key optimizations include:\n\n1. **Multi-threaded data loading**: `num_workers=4` allows CPU to prepare batches while GPU processes\n2. **Pin memory**: `pin_memory=True` speeds up CPU-to-GPU transfer\n3. **Adaptive batch size**: We start with batch_size=8 and reduce if GPU memory errors occur\n\n**Quality control**: We track the number of failed images (corrupted downloads, unusual dimensions) and log them for manual review. In our run, <0.5% of images failed processing.\n\n---\n\n## Feature Distributions\n\n### Comparing Happiness Points vs. Road Samples\n\nAfter processing all images, we can compare the visual characteristics of happiness points against the baseline road samples:\n\n::: {#5d675205 .cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\nfrom scipy import stats\n\ndef compare_distributions(df):\n    \"\"\"\n    Statistical comparison of visual features between groups.\n    \"\"\"\n    happy = df[df['is_happy'] == 1]\n    other = df[df['is_happy'] == 0]\n    \n    features = ['sky_ratio', 'green_view_index', 'building_ratio', \n                'road_ratio', 'vehicle_ratio', 'person_ratio']\n    \n    results = []\n    for feat in features:\n        h_mean = happy[feat].mean()\n        o_mean = other[feat].mean()\n        \n        # Independent samples t-test\n        t_stat, p_value = stats.ttest_ind(\n            happy[feat].dropna(), \n            other[feat].dropna()\n        )\n        \n        results.append({\n            'feature': feat,\n            'happy_mean': h_mean,\n            'other_mean': o_mean,\n            'difference': h_mean - o_mean,\n            't_statistic': t_stat,\n            'p_value': p_value,\n            'significant': p_value < 0.05\n        })\n    \n    return pd.DataFrame(results)\n```\n:::\n\n\n### Analysis\n\nThe statistical comparison reveals several interesting patterns:\n\n| Feature | Happy Mean | Other Mean | Difference | p-value |\n|---------|-----------|-----------|------------|---------|\n| sky_ratio | 0.285 | 0.216 | +0.069 | 0.001** |\n| green_view_index | 0.152 | 0.149 | +0.003 | 0.847 |\n| building_ratio | 0.213 | 0.188 | +0.025 | 0.034* |\n| road_ratio | 0.196 | 0.223 | -0.027 | 0.048* |\n| vehicle_ratio | 0.023 | 0.031 | -0.008 | 0.156 |\n| person_ratio | 0.008 | 0.011 | -0.003 | 0.423 |\n\n**Key observations**:\n\n1. **Sky visibility is significantly higher** at happiness points (+6.9 percentage points, p=0.001). This aligns with research on the psychological benefits of open views and natural light.\n\n2. **Building ratio is higher**, suggesting happiness points are located in denser urban areas with more amenities rather than sparse residential neighborhoods.\n\n3. **Road coverage is lower** at happiness points, potentially indicating more pedestrian-friendly environments with less space dedicated to vehicle traffic.\n\n4. **Green view index shows no significant difference**. This surprising result may reflect that vegetation is fairly uniform across Philadelphia, or that small patches of greenery don't substantially differentiate happy from unhappy locations.\n\n---\n\n## Visualization\n\n### Creating Segmentation Visualizations\n\nFor quality assurance and interpretability, we generate visualizations for each happiness point:\n\n::: {#ef098d2b .cell execution_count=6}\n``` {.python .cell-code}\ndef create_visualization(original_image, segmentation, feature_extractor, point_id):\n    \"\"\"\n    Create a side-by-side visualization of original image and segmentation.\n    \"\"\"\n    h, w = segmentation.shape\n    \n    # Create colored segmentation mask\n    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n    \n    for category, class_ids in feature_extractor.categories.items():\n        mask = np.isin(segmentation, class_ids)\n        color = feature_extractor.colors[category]\n        color_mask[mask] = color\n    \n    # Create overlay\n    color_mask_bgr = cv2.cvtColor(color_mask, cv2.COLOR_RGB2BGR)\n    overlay = cv2.addWeighted(original_image, 0.6, color_mask_bgr, 0.4, 0)\n    \n    # Combine: original | segmentation | overlay\n    combined = np.hstack([original_image, color_mask_bgr, overlay])\n    \n    return combined\n```\n:::\n\n\n### Sample Output\n\nThe visualization shows (left to right): original panorama, segmentation mask, and labeled overlay. Below are four representative examples demonstrating different urban environments:\n\n#### Example 1: High Happiness Score - Urban Park\n\n![Segmentation: Urban park with high greenery](../images/jJkm1xU1q2wDq3tZ6HIXKA_seg.jpg){width=100%}\n\nThis happiness point features abundant vegetation (dark and light green), significant sky visibility (blue), moderate building presence (gray), and pedestrian-friendly paths (light gray). The Green View Index here exceeds 23%, and sky ratio is over 24%—both well above city averages. This type of environment exemplifies the \"ideal\" happy place: open, green, and human-scaled.\n\n#### Example 2: High Happiness Score - Community Garden\n\n![Segmentation: Residential street with community garden](../images/7fxU5aDxWqvtI3lmblf2RQ_seg.jpg){width=100%}\n\nThis happiness point shows a residential street with a vibrant community garden. The segmentation captures: vegetation from the garden and street trees (green), row houses (gray), moderate sky visibility, and limited road surface. The presence of visible human-scale elements (plants, stoops, small buildings) creates an intimate, inviting atmosphere.\n\n#### Example 3: Moderate Score - Urban Commercial District\n\n![Segmentation: Urban commercial street](../images/gkG_zMm_kd5sezfg4OcyCw_seg.jpg){width=100%}\n\nThis University City location shows a typical urban commercial street. Building ratio is high (~22%), sky visibility is moderate, and there's notable vehicle presence (red). The curved building facades and street trees provide visual interest, but the significant road coverage (~42%) and parked vehicles reduce the happiness score compared to pedestrian-oriented spaces.\n\n#### Example 4: Low Happiness Score - Highway/Infrastructure\n\n![Segmentation: Highway with high road ratio](../images/177Csfd8e6xqewWrF-dq5A_seg.jpg){width=100%}\n\nThis location exemplifies features our model associates with *low* happiness: dominant road coverage (~42%), multiple vehicles, minimal vegetation (only 0.2% green), and despite high sky visibility (57%), the environment feels hostile to pedestrians. The segmentation clearly shows why car-centric infrastructure negatively impacts urban happiness—even with open sky, the space is designed for vehicles, not people.\n\n### Comparative Analysis\n\nThese four examples illustrate how segmentation quantifies environmental differences:\n\n| Location | Sky Ratio | Green Index | Building | Road | Vehicle |\n|----------|-----------|-------------|----------|------|---------|\n| Urban Park | 24.3% | 23.8% | 2.0% | 25.1% | 0.0% |\n| Community Garden | 31.2% | 12.8% | 33.5% | 30.1% | 1.8% |\n| Commercial District | 22.5% | 4.1% | 21.9% | 42.6% | 3.2% |\n| Highway | 57.5% | 0.2% | 3.0% | 42.0% | 4.5% |\n\nThe pattern is clear: happiness points have more greenery, less road, and less vehicle presence—regardless of sky ratio.\n\n---\n\n## Limitations\n\nSeveral limitations affect our feature extraction:\n\n1. **Sky ratio vs. Sky View Factor**: We measure simple sky pixel proportion, not the more sophisticated Sky View Factor (SVF) that accounts for hemispherical projection. SVF better represents human perception of \"openness.\"\n\n2. **Static representation**: Images capture a single moment. Time-of-day, seasonal, and weather variations are not captured.\n\n3. **Occlusion**: Objects in the foreground may occlude important background features. A parked truck could hide a beautiful park.\n\n4. **Model limitations**: SegFormer occasionally misclassifies objects, particularly at boundaries or for unusual urban elements (street art, construction zones).\n\n---\n\n## Summary\n\nThis stage transformed ~30,000 street view images into a structured dataset of 6 visual features per location. Key findings include:\n\n- Happiness points have significantly **more sky visibility** and **higher building density**\n- Happiness points have **less road coverage**, suggesting pedestrian-friendly environments\n- Green view index shows **no significant difference**, warranting further investigation\n\nThese visual features, combined with Census socioeconomic data, form the input for our PU Learning model.\n\n",
    "supporting": [
      "2-semantic-segmentation_files"
    ],
    "filters": [],
    "includes": {}
  }
}