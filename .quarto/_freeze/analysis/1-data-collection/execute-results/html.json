{
  "hash": "dda219f879a8553a7d6c7a942ff28efd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Data Collection\"\nsubtitle: \"Sampling Strategy, Street View Download, and Census Integration\"\n---\n\n## Introduction\n\nThe foundation of any spatial analysis lies in the quality and comprehensiveness of its data. This section documents our data collection pipeline, which integrates three distinct data sources: road network data for systematic sampling, Google Street View imagery for visual features, and Census data for socioeconomic context.\n\n## Study Area\n\nOur study focuses on Philadelphia, Pennsylvania—a city with diverse neighborhoods ranging from dense urban cores to quiet residential suburbs. This diversity makes it an ideal location to study how different urban environments relate to perceived happiness.\n\n### Happiness Points Distribution\n\nThe 28 happiness points collected from Drexel University students show a distinct spatial pattern:\n\n![Distribution of Happiness Points in Philadelphia](../images/happiness_points_map.png){width=100%}\n\n**Spatial observations:**\n\n- **Clustering around Center City and University City**: Most happiness points concentrate in the urban core, near commercial districts, cultural institutions, and the university campus\n- **Waterfront presence**: Several points align along the Delaware and Schuylkill Rivers\n- **Few suburban locations**: Very few happiness points appear in outer residential neighborhoods, despite these areas comprising most of Philadelphia's land area\n\nThis clustering pattern already hints at our later finding: happiness is associated with urban vitality rather than quiet residential settings.\n\n## Step 1: Generating Sampling Points\n\n### Methodology\n\nFollowing the approach established by Li & Ratti (2019), we generate sampling points at regular intervals along road centerlines. This systematic approach ensures:\n\n1. **Comprehensive coverage** of the city's street network\n2. **Consistent spatial distribution** avoiding clustering bias\n3. **Reproducibility** of the sampling strategy\n\nWe chose a **200-meter interval** as a balance between data volume and computational feasibility. The projection system uses **NAD 1983 Pennsylvania South (EPSG:2272)** to ensure accurate distance calculations.\n\n### Implementation\n\n::: {#5927db32 .cell execution_count=1}\n``` {.python .cell-code}\nimport geopandas as gpd\nimport pandas as pd\nfrom shapely.geometry import Point\nfrom pathlib import Path\n\ndef generate_sampling_points(centerline_path, boundary_path, interval_m=200):\n    \"\"\"\n    Generate sampling points along road centerlines at regular intervals.\n    \n    Parameters:\n    -----------\n    centerline_path : str\n        Path to road centerline shapefile\n    boundary_path : str\n        Path to city boundary shapefile\n    interval_m : float\n        Sampling interval in meters (default: 200)\n    \n    Returns:\n    --------\n    GeoDataFrame with sampling points\n    \"\"\"\n    # Define projection - NAD 1983 Pennsylvania South (ft)\n    target_crs = \"EPSG:2272\"\n    \n    # Read and project data\n    centerline = gpd.read_file(centerline_path).to_crs(target_crs)\n    boundary = gpd.read_file(boundary_path).to_crs(target_crs)\n    \n    # Clip roads to city boundary\n    centerline_clipped = gpd.clip(centerline, boundary)\n    \n    # Convert interval from meters to feet\n    interval_ft = interval_m * 3.28084\n    \n    # Generate points along each road segment\n    sample_points = []\n    sample_attributes = []\n    \n    for idx, row in centerline_clipped.iterrows():\n        line = row.geometry\n        \n        if line.geom_type == 'LineString':\n            _process_linestring(line, idx, interval_ft, sample_points, sample_attributes)\n        elif line.geom_type == 'MultiLineString':\n            for sub_idx, single_line in enumerate(line.geoms):\n                _process_linestring(single_line, f\"{idx}_{sub_idx}\", interval_ft, \n                                   sample_points, sample_attributes)\n    \n    # Create GeoDataFrame\n    gdf = gpd.GeoDataFrame(sample_attributes, geometry=sample_points, crs=target_crs)\n    gdf['point_id'] = range(1, len(gdf) + 1)\n    \n    # Add WGS84 coordinates for API calls\n    gdf_wgs84 = gdf.to_crs(\"EPSG:4326\")\n    gdf['longitude'] = gdf_wgs84.geometry.x\n    gdf['latitude'] = gdf_wgs84.geometry.y\n    \n    return gdf\n\n\ndef _process_linestring(line, street_id, interval_ft, points_list, attrs_list):\n    \"\"\"Helper function to process a single LineString geometry.\"\"\"\n    line_length = line.length\n    num_points = int(line_length / interval_ft) + 1\n    \n    for i in range(num_points):\n        distance = i * interval_ft\n        if distance <= line_length:\n            point = line.interpolate(distance)\n            points_list.append(point)\n            attrs_list.append({\n                'point_type': 'road_sample',\n                'street_id': street_id,\n                'distance_ft': round(distance, 2)\n            })\n```\n:::\n\n\n### Analysis\n\nThe sampling algorithm produced **approximately 30,000 points** distributed across Philadelphia's street network. Several important considerations emerged during implementation:\n\n1. **Geometry handling**: Philadelphia's road data contains both `LineString` and `MultiLineString` geometries. Multi-part geometries occur where a single road record spans multiple disconnected segments (e.g., roads split by parks or rivers). We process each component separately to maintain accurate sampling.\n\n2. **Projection choice**: Using a local projected coordinate system (EPSG:2272) rather than WGS84 is critical. At Philadelphia's latitude, WGS84 coordinates would introduce ~15% error in distance calculations due to meridian convergence.\n\n3. **Edge effects**: Roads at the city boundary may be partially outside the study area. By clipping to the boundary first, we ensure all sampling points fall within Philadelphia.\n\n### Results Summary\n\n| Metric | Value |\n|--------|-------|\n| Total road segments | ~47,000 |\n| Sampling interval | 200 meters |\n| Generated points | ~30,000 |\n| Coverage area | 142 sq mi |\n\n---\n\n## Step 2: Google Street View Download\n\n### Technical Approach\n\nGoogle Street View provides the visual data for our analysis. We use the **tile-based download method** rather than the Static API to obtain high-resolution panoramic images.\n\nKey specifications:\n\n- **Resolution**: 3328 × 1664 pixels (zoom level 3)\n- **Tile grid**: 7 columns × 4 rows\n- **Image format**: JPEG, quality 95%\n\n### Implementation\n\n::: {#8a32ddf3 .cell execution_count=2}\n``` {.python .cell-code}\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport time\n\nclass GSVDownloader:\n    \"\"\"\n    Google Street View panorama downloader using tile stitching method.\n    \n    This approach downloads individual tiles and stitches them together,\n    providing higher resolution than the standard Static API limit.\n    \"\"\"\n    \n    def __init__(self, api_key):\n        self.api_key = api_key\n        self.metadata_url = \"https://maps.googleapis.com/maps/api/streetview/metadata\"\n        \n        # Tile configuration for each zoom level\n        self.grid_sizes = {\n            0: (1, 1),    # 512 x 512\n            1: (2, 1),    # 1024 x 512\n            2: (4, 2),    # 2048 x 1024\n            3: (7, 4),    # 3584 x 2048 -> cropped to 3328 x 1664\n            4: (13, 7),   # 6656 x 3584\n            5: (26, 13)   # 13312 x 6656\n        }\n    \n    def get_metadata(self, lat, lon):\n        \"\"\"\n        Retrieve GSV metadata for a location.\n        \n        Returns pano_id, capture date, and actual location if available.\n        \"\"\"\n        params = {\n            'location': f\"{lat},{lon}\",\n            'key': self.api_key\n        }\n        \n        response = requests.get(self.metadata_url, params=params, timeout=10)\n        data = response.json()\n        \n        if data.get('status') == 'OK':\n            return {\n                'pano_id': data['pano_id'],\n                'lat': data['location']['lat'],\n                'lon': data['location']['lng'],\n                'date': data.get('date'),\n                'status': 'OK'\n            }\n        return {'status': data.get('status', 'ERROR')}\n    \n    def download_panorama(self, pano_id, zoom=3, output_path=None):\n        \"\"\"\n        Download and stitch panorama tiles.\n        \n        Parameters:\n        -----------\n        pano_id : str\n            Google panorama ID\n        zoom : int\n            Zoom level (0-5), higher = more detail\n        output_path : Path\n            Where to save the stitched image\n        \"\"\"\n        tile_size = 512\n        cols, rows = self.grid_sizes[zoom]\n        \n        # Create blank canvas\n        panorama = Image.new('RGB', (cols * tile_size, rows * tile_size))\n        \n        # Download and place each tile\n        for y in range(rows):\n            for x in range(cols):\n                tile_url = (f\"https://cbk0.google.com/cbk?\"\n                           f\"output=tile&panoid={pano_id}&zoom={zoom}&x={x}&y={y}\")\n                \n                try:\n                    response = requests.get(tile_url, timeout=10)\n                    if response.status_code == 200:\n                        tile = Image.open(BytesIO(response.content))\n                        panorama.paste(tile, (x * tile_size, y * tile_size))\n                except Exception as e:\n                    print(f\"Failed to download tile ({x},{y}): {e}\")\n                \n                time.sleep(0.02)  # Rate limiting\n        \n        if output_path:\n            panorama.save(output_path, 'JPEG', quality=95)\n        \n        return panorama\n```\n:::\n\n\n### Analysis\n\nThe tile-based approach offers several advantages over the standard Street View Static API:\n\n1. **Higher resolution**: The Static API limits images to 640×640 pixels. By downloading tiles at zoom level 3, we achieve 3328×1664 pixels—over 8× more pixels for semantic segmentation.\n\n2. **No watermarks**: Direct tile access produces clean images without the Google logo overlay that appears on Static API responses.\n\n3. **Cost efficiency**: Tile requests don't count against the paid Static API quota.\n\n**Coverage analysis** revealed that approximately **98% of sampling points** have available Street View imagery. The ~2% without coverage typically fall in:\n\n- Recently developed areas\n- Private roads or restricted access zones\n- Parks and green spaces without street access\n\n### Download Statistics\n\n| Metric | Value |\n|--------|-------|\n| Points with GSV | ~29,500 |\n| Coverage rate | ~98% |\n| Image dimensions | 3328 × 1664 px |\n| Total storage | ~45 GB |\n\n### Sample Street View Imagery\n\nBelow are examples of downloaded panoramas showing the diversity of urban environments in Philadelphia:\n\n::: {layout-ncol=2}\n\n![Urban park environment (Happiness Point)](../images/jJkm1xU1q2wDq3tZ6HIXKA.jpg)\n\n![Community garden street (Happiness Point)](../images/7fxU5aDxWqvtI3lmblf2RQ.jpg)\n\n:::\n\n::: {layout-ncol=2}\n\n![Commercial district](../images/gkG_zMm_kd5sezfg4OcyCw.jpg)\n\n![Highway infrastructure](../images/177Csfd8e6xqewWrF-dq5A.jpg)\n\n:::\n\nThese examples illustrate the visual diversity captured in our dataset—from lush parks and intimate residential streets to commercial corridors and highway infrastructure. This variation is essential for building a model that can distinguish environmental characteristics associated with happiness.\n\n---\n\n## Step 3: Happiness Point Extraction\n\n### The Challenge\n\nHappiness points present a unique challenge: they were marked by survey respondents at locations they personally associate with happiness, which may not align perfectly with street-accessible viewpoints. Some locations include:\n\n- Private gardens or courtyards\n- Building rooftops\n- Indoor locations (cafes, libraries)\n- Park interiors\n\n### Implementation\n\n::: {#2a7cf379 .cell execution_count=3}\n``` {.python .cell-code}\nimport shutil\n\ndef extract_happiness_images(sampling_gdf, metadata_df, source_dir, target_dir):\n    \"\"\"\n    Extract GSV images corresponding to happiness points.\n    \n    Implements a tiered matching strategy:\n    1. Direct match using point coordinates\n    2. Expanded radius search for nearby panoramas\n    3. Manual verification for remaining points\n    \"\"\"\n    # Filter happiness points\n    happy_points = sampling_gdf[sampling_gdf['point_type'] == 'happy_point']\n    happy_ids = set(happy_points['point_id'])\n    \n    print(f\"Processing {len(happy_ids)} happiness points...\")\n    \n    # Match with metadata\n    happy_metadata = metadata_df[metadata_df['point_id'].isin(happy_ids)]\n    \n    matched_count = 0\n    missing_points = []\n    \n    for _, row in happy_metadata.iterrows():\n        pano_id = row['pano_id']\n        source_file = source_dir / f\"{pano_id}.jpg\"\n        target_file = target_dir / f\"{pano_id}.jpg\"\n        \n        if source_file.exists():\n            shutil.copy2(source_file, target_file)\n            matched_count += 1\n        else:\n            missing_points.append(row['point_id'])\n    \n    print(f\"Matched: {matched_count}\")\n    print(f\"Missing: {len(missing_points)}\")\n    \n    return matched_count, missing_points\n```\n:::\n\n\n### Analysis\n\nOur extraction process revealed an important methodological consideration: **the street view visible from a road may not capture what makes a location \"happy\"** for respondents.\n\nFor example, consider a happiness point marked at a hidden courtyard garden. The nearest street view shows only the building's exterior wall. The actual source of happiness—the garden itself—is invisible to our analysis.\n\nThis limitation affects **approximately 6 of the 28 happiness points** (21%):\n\n| Match Type | Count | Percentage |\n|------------|-------|------------|\n| Direct match | 22 | 79% |\n| Expanded radius | 4 | 14% |\n| No suitable view | 2 | 7% |\n\n---\n\n## Step 4: Census Data Integration\n\n### Variable Selection\n\nWe selected Census variables that prior research has linked to neighborhood quality and perceived well-being:\n\n::: {#7141d8bd .cell execution_count=4}\n``` {.python .cell-code}\nclass CensusDownloader:\n    \"\"\"\n    Download and process Census ACS 5-year estimates.\n    \n    Focuses on variables related to:\n    - Demographics (age, race)\n    - Economic status (income, poverty)\n    - Education\n    - Housing characteristics\n    \"\"\"\n    \n    def __init__(self, api_key):\n        self.api_key = api_key\n        self.base_url = \"https://api.census.gov/data\"\n        \n        # Philadelphia: State 42 (PA), County 101\n        self.state = \"42\"\n        self.county = \"101\"\n        \n        # ACS variables of interest\n        self.variables = {\n            # Population\n            'B01003_001E': 'total_pop',\n            'B01002_001E': 'median_age',\n            \n            # Race\n            'B02001_002E': 'pop_white',\n            'B02001_003E': 'pop_black',\n            \n            # Income & Poverty\n            'B19013_001E': 'median_income',\n            'B17001_002E': 'pop_poverty',\n            \n            # Education (25+)\n            'B15003_022E': 'pop_bachelor',\n            'B15003_023E': 'pop_master',\n            'B15003_025E': 'pop_doctorate',\n            \n            # Housing\n            'B25003_001E': 'total_housing',\n            'B25003_002E': 'owner_occupied',\n            \n            # Employment\n            'B23025_003E': 'labor_force',\n            'B23025_005E': 'unemployed',\n        }\n    \n    def download(self, year=2022):\n        \"\"\"Download ACS 5-year estimates for Philadelphia Census Tracts.\"\"\"\n        var_string = ','.join(self.variables.keys())\n        \n        url = f\"{self.base_url}/{year}/acs/acs5\"\n        params = {\n            'get': f'NAME,{var_string}',\n            'for': 'tract:*',\n            'in': f'state:{self.state} county:{self.county}',\n            'key': self.api_key\n        }\n        \n        response = requests.get(url, params=params, timeout=60)\n        data = response.json()\n        \n        # Convert to DataFrame\n        df = pd.DataFrame(data[1:], columns=data[0])\n        \n        # Rename columns\n        for code, name in self.variables.items():\n            if code in df.columns:\n                df = df.rename(columns={code: name})\n        \n        # Create GEOID for spatial join\n        df['GEOID'] = df['state'] + df['county'] + df['tract']\n        \n        return df\n    \n    def calculate_rates(self, df):\n        \"\"\"Calculate derived variables (percentages and rates).\"\"\"\n        # Safely divide, handling zeros\n        def safe_div(num, denom):\n            return num / denom.where(denom > 0)\n        \n        df['pct_white'] = safe_div(df['pop_white'], df['total_pop'])\n        df['poverty_rate'] = safe_div(df['pop_poverty'], df['total_pop'])\n        \n        # College education rate\n        df['pop_college'] = (df['pop_bachelor'].fillna(0) + \n                            df['pop_master'].fillna(0) + \n                            df['pop_doctorate'].fillna(0))\n        df['pct_college'] = safe_div(df['pop_college'], df['total_pop'])\n        \n        df['pct_owner_occupied'] = safe_div(df['owner_occupied'], df['total_housing'])\n        df['unemployment_rate'] = safe_div(df['unemployed'], df['labor_force'])\n        \n        return df\n```\n:::\n\n\n### Analysis\n\nThe Census data integration revealed several important patterns in Philadelphia's socioeconomic geography:\n\n**Data quality issue**: The Census API returns **-666666666** for missing or suppressed values (typically in tracts with very small populations where releasing data would compromise anonymity). We identified and replaced these with `NaN` before analysis.\n\n**Spatial resolution limitation**: Census Tracts are relatively large geographic units—Philadelphia has 384 tracts for a city of 1.58 million people. This means each tract contains approximately 4,100 residents on average, and any sampling point within a tract receives the same socioeconomic values.\n\nThis creates an **ecological fallacy risk**: we're assigning tract-level characteristics to point-level observations. A street corner in a wealthy tract may actually be surrounded by lower-income residents, but our data cannot capture this intra-tract variation.\n\n### Census Summary Statistics\n\n| Variable | Min | Mean | Max |\n|----------|-----|------|-----|\n| Median Income | $13,125 | $52,847 | $250,000+ |\n| Poverty Rate | 0.0% | 22.4% | 68.2% |\n| College Rate | 2.1% | 31.2% | 89.4% |\n| Owner-Occupied | 0.0% | 52.3% | 97.8% |\n\n---\n\n## Final Dataset\n\nAfter completing all data collection steps, we produce a unified dataset containing:\n\n- **~30,000 observations** (sampling points)\n- **6 visual features** (from semantic segmentation, covered next)\n- **8 socioeconomic features** (from Census)\n- **Binary label**: happiness point (1) or road sample (0)\n\nThis dataset forms the foundation for our PU Learning analysis.\n\n",
    "supporting": [
      "1-data-collection_files"
    ],
    "filters": [],
    "includes": {}
  }
}