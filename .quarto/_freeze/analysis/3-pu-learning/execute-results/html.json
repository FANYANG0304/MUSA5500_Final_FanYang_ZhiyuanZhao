{
  "hash": "bb7aeb9e7c013fd70fb9442ec77d83e1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"PU Learning\"\nsubtitle: \"Modeling Happiness with Positive-Unlabeled Learning\"\n---\n\n## Introduction\n\nThis section presents the core machine learning methodology: **Positive-Unlabeled (PU) Learning**. This framework is specifically designed for classification problems where we have labeled positive examples but no confirmed negative examples—exactly our situation with happiness points.\n\n## Problem Formulation\n\n### Why Not Standard Classification?\n\nIn traditional binary classification, we need both positive and negative labeled examples. Our dataset contains:\n\n- **Positive (labeled)**: 28 happiness points marked by survey respondents\n- **Unlabeled**: ~30,000 road sampling points with unknown happiness status\n\nThe critical insight is that **unlabeled does not mean negative**. Many road sampling points might also be \"happy\" locations—they simply weren't identified in the survey. Training a standard classifier with all unlabeled points as negatives would systematically bias the model.\n\n### The PU Learning Approach\n\nPU Learning addresses this by:\n\n1. **Identifying reliable negatives** from the unlabeled set\n2. **Training classifiers** using positive samples + reliable negatives\n3. **Scoring all points** to predict happiness probability\n\n::: {#f42844b1 .cell execution_count=1}\n``` {.python .cell-code}\nfrom scipy.spatial.distance import cdist\nimport numpy as np\n\nclass PULearningModel:\n    \"\"\"\n    Positive-Unlabeled Learning classifier for happiness prediction.\n    \n    Uses distance-based reliable negative identification followed by\n    standard classification with class balancing.\n    \"\"\"\n    \n    def __init__(self, reliable_neg_ratio=0.30):\n        \"\"\"\n        Parameters:\n        -----------\n        reliable_neg_ratio : float\n            Proportion of unlabeled samples to use as reliable negatives.\n            Higher values increase training set size but risk label noise.\n        \"\"\"\n        self.reliable_neg_ratio = reliable_neg_ratio\n        self.scaler = StandardScaler()\n```\n:::\n\n\n---\n\n## Feature Engineering\n\n### Feature Set\n\nWe construct a feature vector combining visual and socioeconomic characteristics:\n\n::: {#4d4ad916 .cell execution_count=2}\n``` {.python .cell-code}\n# Visual features from semantic segmentation\nvisual_features = [\n    'sky_ratio',           # Sky visibility\n    'green_view_index',    # Vegetation coverage\n    'building_ratio',      # Built environment density\n    'road_ratio',          # Road/sidewalk coverage\n    'vehicle_ratio',       # Vehicle presence\n    'person_ratio'         # Pedestrian presence\n]\n\n# Socioeconomic features from Census\ncensus_features = [\n    'median_income',       # Economic status\n    'poverty_rate',        # Deprivation indicator\n    'pct_college',         # Education level\n    'pct_white',           # Racial composition\n    'median_age',          # Age demographics\n    'pct_owner_occupied',  # Housing tenure\n    'unemployment_rate'    # Employment status\n]\n\nall_features = visual_features + census_features\n```\n:::\n\n\n### Analysis\n\nThe feature set was designed to capture multiple dimensions of neighborhood quality:\n\n**Visual features** directly measure what a person sees when standing at a location. Prior research (Naik et al., 2014; Salesses et al., 2013) has shown that visual characteristics correlate with perceived safety and pleasantness.\n\n**Socioeconomic features** capture the broader neighborhood context that may influence how places \"feel\"—even if not directly visible. A clean street in a high-income neighborhood may feel different from an identical street in a high-poverty area due to ambient factors (noise, social activity, maintenance levels).\n\n**Feature selection considerations**: We intentionally excluded highly correlated features (e.g., using `pct_college` rather than separate counts for each degree level) to avoid multicollinearity issues in logistic regression.\n\n---\n\n## Reliable Negative Identification\n\n### Distance-Based Method\n\nOur approach identifies reliable negatives based on feature-space distance from positive examples:\n\n::: {#7aaa9da3 .cell execution_count=3}\n``` {.python .cell-code}\ndef identify_reliable_negatives(self, X_positive, X_unlabeled):\n    \"\"\"\n    Identify reliable negative samples using distance to positive centroid.\n    \n    Intuition: Points far from the positive centroid in feature space\n    are unlikely to be latent positives.\n    \"\"\"\n    # Calculate centroid of positive samples\n    positive_centroid = X_positive.mean(axis=0)\n    \n    # Calculate Euclidean distance from each unlabeled point to centroid\n    distances = cdist(X_unlabeled, [positive_centroid], metric='euclidean').flatten()\n    \n    # Select most distant points as reliable negatives\n    n_reliable = int(len(X_unlabeled) * self.reliable_neg_ratio)\n    reliable_neg_indices = np.argsort(distances)[-n_reliable:]\n    \n    # Also identify \"suspect positives\" (very close to positive centroid)\n    n_suspect = int(len(X_unlabeled) * 0.05)\n    suspect_pos_indices = np.argsort(distances)[:n_suspect]\n    \n    return reliable_neg_indices, suspect_pos_indices, distances\n```\n:::\n\n\n### Analysis\n\nThe distance-based method rests on a key assumption: **happiness points share common characteristics that cluster them together in feature space**.\n\nWe chose **30% as the reliable negative ratio** based on:\n\n1. **Balancing label noise**: Too few negatives limits training data; too many risks including latent positives\n2. **Prior estimates**: Urban planning literature suggests ~60-70% of city locations are \"neutral\"—neither particularly happy nor unhappy\n3. **Empirical testing**: Cross-validation performance plateaued around 25-35%\n\nThe **5% suspect positive** identification serves two purposes:\n\n- These points are excluded from the reliable negative set as a safety margin\n- They can be examined post-hoc to discover potentially overlooked happy locations\n\n### Visualization of Sample Distribution\n\n::: {#fe2c0873 .cell execution_count=4}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef visualize_sample_distribution(X_pos, X_unl, reliable_idx, suspect_idx):\n    \"\"\"\n    2D PCA visualization of sample distribution.\n    \"\"\"\n    # Combine and project to 2D\n    X_all = np.vstack([X_pos, X_unl])\n    pca = PCA(n_components=2)\n    X_2d = pca.fit_transform(X_all)\n    \n    n_pos = len(X_pos)\n    \n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    # Plot each category\n    # Reliable negatives\n    ax.scatter(X_2d[n_pos:][reliable_idx, 0], X_2d[n_pos:][reliable_idx, 1],\n               c='blue', alpha=0.3, s=10, label='Reliable Negative')\n    \n    # Suspect positives\n    ax.scatter(X_2d[n_pos:][suspect_idx, 0], X_2d[n_pos:][suspect_idx, 1],\n               c='orange', alpha=0.5, s=20, label='Suspect Positive')\n    \n    # Labeled positives\n    ax.scatter(X_2d[:n_pos, 0], X_2d[:n_pos, 1],\n               c='red', s=100, marker='*', label='Labeled Positive')\n    \n    ax.legend()\n    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n    ax.set_title('PCA Projection of Sample Distribution')\n    \n    return fig\n```\n:::\n\n\n---\n\n## Model Training\n\n### Classifier Selection\n\nWe train two complementary classifiers:\n\n::: {#b46eadc2 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\ndef train_models(self, X_positive, X_unlabeled):\n    \"\"\"\n    Train PU Learning classifiers.\n    \"\"\"\n    # Scale features\n    X_all = np.vstack([X_positive, X_unlabeled])\n    self.scaler.fit(X_all)\n    \n    X_pos_scaled = self.scaler.transform(X_positive)\n    X_unl_scaled = self.scaler.transform(X_unlabeled)\n    \n    # Identify reliable negatives\n    reliable_idx, suspect_idx, distances = self.identify_reliable_negatives(\n        X_pos_scaled, X_unl_scaled\n    )\n    \n    # Build training set\n    X_reliable_neg = X_unl_scaled[reliable_idx]\n    X_train = np.vstack([X_pos_scaled, X_reliable_neg])\n    y_train = np.concatenate([\n        np.ones(len(X_pos_scaled)),\n        np.zeros(len(X_reliable_neg))\n    ])\n    \n    # Logistic Regression - for interpretability\n    self.lr_model = LogisticRegression(\n        class_weight='balanced',  # Handle class imbalance\n        max_iter=1000,\n        random_state=42\n    )\n    self.lr_model.fit(X_train, y_train)\n    \n    # Random Forest - for non-linear patterns\n    self.rf_model = RandomForestClassifier(\n        n_estimators=200,\n        max_depth=10,\n        min_samples_leaf=5,\n        class_weight='balanced',\n        random_state=42,\n        n_jobs=-1\n    )\n    self.rf_model.fit(X_train, y_train)\n    \n    # Cross-validation\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    lr_scores = cross_val_score(self.lr_model, X_train, y_train, \n                                 cv=cv, scoring='roc_auc')\n    rf_scores = cross_val_score(self.rf_model, X_train, y_train,\n                                 cv=cv, scoring='roc_auc')\n    \n    return {\n        'lr_auc': (lr_scores.mean(), lr_scores.std()),\n        'rf_auc': (rf_scores.mean(), rf_scores.std())\n    }\n```\n:::\n\n\n### Analysis\n\n**Why two classifiers?**\n\n- **Logistic Regression** provides interpretable coefficients showing the direction and magnitude of each feature's effect\n- **Random Forest** can capture non-linear relationships and interactions that logistic regression misses\n\n**Class weighting** (`class_weight='balanced'`) is essential because even after PU Learning, we have a 28:~9,000 class ratio. Without weighting, the model would achieve high accuracy by predicting all points as negative.\n\n**Cross-validation strategy**: We use 5-fold stratified cross-validation, ensuring each fold contains proportional representation of positive samples (about 5-6 per fold). This is the minimum viable setup given our small positive sample size.\n\n---\n\n## Model Evaluation\n\n### ROC Curve Analysis\n\n::: {#8cdf8882 .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.metrics import roc_curve, auc\n\ndef plot_roc_curves(self, X_train, y_train):\n    \"\"\"\n    Generate ROC curves with 5-fold cross-validation.\n    \"\"\"\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    fig, ax = plt.subplots(figsize=(8, 8))\n    mean_fpr = np.linspace(0, 1, 100)\n    \n    lr_tprs = []\n    rf_tprs = []\n    \n    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n        X_tr, y_tr = X_train[train_idx], y_train[train_idx]\n        X_val, y_val = X_train[val_idx], y_train[val_idx]\n        \n        # Logistic Regression\n        lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n        lr.fit(X_tr, y_tr)\n        lr_prob = lr.predict_proba(X_val)[:, 1]\n        lr_fpr, lr_tpr, _ = roc_curve(y_val, lr_prob)\n        lr_tprs.append(np.interp(mean_fpr, lr_fpr, lr_tpr))\n        \n        # Random Forest\n        rf = RandomForestClassifier(n_estimators=200, max_depth=10, \n                                    class_weight='balanced', random_state=42)\n        rf.fit(X_tr, y_tr)\n        rf_prob = rf.predict_proba(X_val)[:, 1]\n        rf_fpr, rf_tpr, _ = roc_curve(y_val, rf_prob)\n        rf_tprs.append(np.interp(mean_fpr, rf_fpr, rf_tpr))\n    \n    # Plot mean curves\n    ax.plot(mean_fpr, np.mean(lr_tprs, axis=0), \n            label=f'Logistic Regression (AUC = {np.mean([auc(mean_fpr, t) for t in lr_tprs]):.3f})')\n    ax.plot(mean_fpr, np.mean(rf_tprs, axis=0),\n            label=f'Random Forest (AUC = {np.mean([auc(mean_fpr, t) for t in rf_tprs]):.3f})')\n    ax.plot([0, 1], [0, 1], '--', color='gray', label='Random')\n    \n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.legend()\n    \n    return fig\n```\n:::\n\n\n### Results\n\n![ROC Curves from 5-Fold Cross-Validation](../images/roc_curve.png){width=80%}\n\n| Model | AUC | Std Dev |\n|-------|-----|---------|\n| Logistic Regression | 0.968 | ±0.013 |\n| Random Forest | 0.939 | ±0.045 |\n\n### Analysis\n\nBoth models achieve strong discriminative performance, with **Logistic Regression slightly outperforming Random Forest**.\n\n**Interpreting high AUC**: An AUC of 0.968 means that if we randomly select one positive and one negative sample, the model correctly ranks them 96.8% of the time. This suggests that happiness points are indeed distinguishable from reliable negatives in feature space.\n\n**Why Logistic Regression beats Random Forest**: With only 28 positive samples, Random Forest's flexibility becomes a liability—it can overfit to idiosyncratic patterns in the small positive set. Logistic regression's stronger inductive bias (linear decision boundary) provides regularization that helps generalization.\n\n**Variance comparison**: Logistic regression shows lower variance across folds (±0.013 vs ±0.045), indicating more stable performance.\n\n---\n\n## Feature Importance\n\n### Logistic Regression Coefficients\n\n::: {#615ef1d6 .cell execution_count=7}\n``` {.python .cell-code}\ndef get_feature_importance(self, feature_names):\n    \"\"\"\n    Extract and format feature importance from both models.\n    \"\"\"\n    importance = pd.DataFrame({\n        'feature': feature_names,\n        'lr_coefficient': self.lr_model.coef_[0],\n        'rf_importance': self.rf_model.feature_importances_\n    })\n    \n    importance['direction'] = importance['lr_coefficient'].apply(\n        lambda x: 'Positive (+)' if x > 0 else 'Negative (-)'\n    )\n    importance['lr_abs'] = importance['lr_coefficient'].abs()\n    \n    return importance.sort_values('lr_abs', ascending=False)\n```\n:::\n\n\n### Results\n\n| Feature | LR Coefficient | Direction | RF Importance |\n|---------|---------------|-----------|---------------|\n| pct_owner_occupied | -2.10 | Negative | 0.089 |\n| poverty_rate | -1.28 | Negative | 0.076 |\n| road_ratio | -1.23 | Negative | 0.082 |\n| building_ratio | +1.00 | Positive | 0.095 |\n| pct_college | +0.70 | Positive | 0.088 |\n| sky_ratio | +0.58 | Positive | 0.091 |\n| vehicle_ratio | -0.29 | Negative | 0.078 |\n| person_ratio | -0.24 | Negative | 0.065 |\n| median_income | +0.18 | Positive | 0.112 |\n| green_view_index | +0.06 | Positive | 0.087 |\n| median_age | -0.04 | Negative | 0.071 |\n| unemployment_rate | -0.02 | Negative | 0.066 |\n\n### Analysis\n\nThe coefficient analysis reveals several substantive findings:\n\n**Strongest negative predictor: Owner-occupancy rate (-2.10)**\n\nThis counterintuitive finding suggests that areas with high homeownership are *less* likely to contain happiness points. Possible explanations:\n\n1. High-ownership areas are often quiet residential suburbs with few destinations\n2. Happiness points may cluster in mixed-use, walkable areas that tend to have more renters\n3. This reflects a distinction between \"good for living\" and \"feels happy to visit\"\n\n**Road ratio as negative predictor (-1.23)**\n\nMore road coverage = less happiness. This aligns with urban design principles that car-centric environments are less pleasant for pedestrians.\n\n**Building ratio as positive predictor (+1.00)**\n\nHigher building density correlates with happiness, likely because dense areas have more amenities, activities, and social opportunities.\n\n**Sky visibility as positive predictor (+0.58)**\n\nEven controlling for building density, more sky visibility predicts happiness. This supports the psychological value of \"openness\" in urban environments.\n\n---\n\n## Predictions\n\n### City-Wide Happiness Scoring\n\n::: {#e60e8287 .cell execution_count=8}\n``` {.python .cell-code}\ndef predict_happiness(self, df, feature_cols):\n    \"\"\"\n    Generate happiness predictions for all points.\n    \"\"\"\n    X = df[feature_cols].values\n    X_scaled = self.scaler.transform(X)\n    \n    # Get probability scores\n    raw_probs = self.lr_model.predict_proba(X_scaled)[:, 1]\n    \n    # Normalize to 0-1 scale\n    normalized = (raw_probs - raw_probs.min()) / (raw_probs.max() - raw_probs.min())\n    \n    # Ensure labeled positives get score 1.0\n    if 'is_happy' in df.columns:\n        normalized[df['is_happy'] == 1] = 1.0\n    \n    df['happiness_score'] = normalized\n    \n    return df\n```\n:::\n\n\n### Score Distribution\n\nAfter scoring all ~30,000 points:\n\n| Score Range | Count | Percentage |\n|-------------|-------|------------|\n| 0.0 - 0.2 | ~9,000 | 30% |\n| 0.2 - 0.4 | ~8,000 | 27% |\n| 0.4 - 0.6 | ~7,000 | 23% |\n| 0.6 - 0.8 | ~4,000 | 13% |\n| 0.8 - 1.0 | ~2,000 | 7% |\n\n### Analysis\n\nThe score distribution is **right-skewed**, with most points receiving low happiness scores. This is expected—if happiness points were common, they wouldn't be special.\n\nThe ~2,000 points (7%) scoring above 0.8 represent locations that share characteristics with labeled happiness points. These could be:\n\n- **True latent positives**: Happy places that weren't identified in the survey\n- **False positives**: Places that statistically resemble happy places but don't evoke happiness\n\nManual validation of a random sample of high-scoring points could help distinguish these cases.\n\n---\n\n## Summary\n\nThe PU Learning approach successfully identifies features that distinguish happiness points:\n\n1. **Methodology**: Distance-based reliable negative identification + class-balanced logistic regression achieves AUC = 0.968\n\n2. **Key positive factors**: Building density, college education rate, sky visibility\n\n3. **Key negative factors**: Owner-occupancy rate, poverty rate, road coverage\n\n4. **Predictions**: Generated happiness scores for all ~30,000 sampling points, enabling city-wide mapping\n\nThe next section presents the full results and discusses implications for urban planning.\n\n",
    "supporting": [
      "3-pu-learning_files"
    ],
    "filters": [],
    "includes": {}
  }
}