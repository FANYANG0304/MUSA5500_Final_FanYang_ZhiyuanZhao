{"title":"Semantic Segmentation","markdown":{"yaml":{"title":"Semantic Segmentation","subtitle":"Extracting Urban Visual Features from Street View Imagery"},"headingText":"From Pixels to Meaning","containsRefs":false,"markdown":"\n\n\nA street view image contains rich information about the urban environment, but raw pixel values tell us nothing directly useful. What we need is a way to answer questions like: How much of this scene is sky? How much is vegetation? Is this a car-dominated environment or a pedestrian-friendly one?\n\n**Semantic segmentation** transforms images into meaningful categories, labeling every pixel with what it represents—sky, tree, building, road, car, person. This transformation is the bridge between visual data and quantitative analysis.\n\n---\n\n## Why SegFormer?\n\nWe selected **SegFormer-B0** (Xie et al., 2021) after considering several alternatives. The model is pre-trained on ADE20K, a dataset containing 150 semantic classes with excellent coverage of urban scenes. The B0 variant represents the smallest in the SegFormer family, but for our purposes this is a feature rather than a limitation: processing ~40,000 images demands efficiency, and SegFormer-B0 delivers roughly 2 images per second on a mid-range GPU while using only ~4GB of VRAM.\n\n```python\n#| eval: false\n\nfrom transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\nimport torch\n\n# Load pre-trained model\nmodel_name = \"nvidia/segformer-b0-finetuned-ade-512-512\"\nprocessor = SegformerImageProcessor.from_pretrained(model_name)\nmodel = SegformerForSemanticSegmentation.from_pretrained(model_name)\n\n# Check available device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\nmodel.eval()\n```\n\nWhile newer models might achieve marginally higher accuracy on benchmarks, SegFormer's transformer-based architecture proves robust across diverse urban scenes—from tree-lined residential streets to concrete highway interchanges. For tens of thousands of images, practical considerations outweigh small accuracy differences.\n\n---\n\n## Defining Urban Categories\n\nADE20K provides 150 fine-grained classes, far more granularity than we need. For urban happiness analysis, we aggregate these into **6 meaningful categories**:\n\n```python\n#| eval: false\n\nclass UrbanFeatureExtractor:\n    \"\"\"\n    Extract urban environmental features from semantic segmentation results.\n    \n    Aggregates ADE20K's 150 classes into 6 urban-relevant categories.\n    \"\"\"\n    \n    def __init__(self):\n        # ADE20K class indices for each urban category\n        self.categories = {\n            'sky': [2],                          # sky\n            'vegetation': [4, 9, 17, 66, 72],    # tree, grass, plant, palm, flower\n            'building': [1, 25, 48, 84],         # building, house, skyscraper, booth\n            'road': [6, 11, 52],                 # road, sidewalk, path\n            'vehicle': [20, 80, 83, 102, 127],   # car, bus, truck, van, bicycle\n            'person': [12]                       # person\n        }\n        \n        # Colors for visualization (RGB)\n        self.colors = {\n            'sky': [135, 206, 235],      # Light blue\n            'vegetation': [34, 139, 34],  # Forest green\n            'building': [128, 128, 128],  # Gray\n            'road': [64, 64, 64],         # Dark gray\n            'vehicle': [255, 0, 0],       # Red\n            'person': [255, 192, 203]     # Pink\n        }\n    \n    def calculate_ratios(self, segmentation_map):\n        \"\"\"\n        Calculate the proportion of each category in the image.\n        \n        Parameters:\n        -----------\n        segmentation_map : numpy array\n            2D array of class predictions (H x W)\n        \n        Returns:\n        --------\n        dict : Category ratios (values sum to less than 1.0 due to uncategorized pixels)\n        \"\"\"\n        total_pixels = segmentation_map.size\n        ratios = {}\n        \n        for category, class_ids in self.categories.items():\n            # Count pixels belonging to this category\n            mask = np.isin(segmentation_map, class_ids)\n            pixel_count = np.sum(mask)\n            ratios[f'{category}_ratio'] = pixel_count / total_pixels\n        \n        # Green View Index is a common metric\n        ratios['green_view_index'] = ratios.pop('vegetation_ratio')\n        \n        return ratios\n```\n\nThe rationale behind these groupings reflects both practical and theoretical considerations. **Sky** remains a single class because it represents a unified visual experience—the sense of openness that comes from seeing unobstructed sky. **Vegetation** combines trees, grass, and plants because prior research on Green View Index treats all greenery similarly for psychological impact (Asgarzadeh et al., 2012). **Buildings** includes multiple structure types because we care about overall built environment density, not architectural distinctions. **Road** encompasses sidewalks and paths as collective transportation infrastructure.\n\nThe remaining pixels—furniture, signage, water, and other elements—typically constitute 5-15% of any given image.\n\n---\n\n## Processing Pipeline\n\n### Single Image Segmentation\n\n```python\n#| eval: false\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\ndef segment_image(image_path, model, processor, device):\n    \"\"\"\n    Perform semantic segmentation on a single street view image.\n    \n    Returns both the segmentation map and calculated feature ratios.\n    \"\"\"\n    # Load and prepare image\n    image = cv2.imread(str(image_path))\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    pil_image = Image.fromarray(image_rgb)\n    \n    # Process through model\n    inputs = processor(images=pil_image, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n    \n    # Upsample to original resolution\n    h, w = image.shape[:2]\n    upsampled = torch.nn.functional.interpolate(\n        logits,\n        size=(h, w),\n        mode='bilinear',\n        align_corners=False\n    )\n    \n    # Get class predictions\n    segmentation = upsampled.argmax(dim=1).squeeze().cpu().numpy()\n    \n    return segmentation, image\n```\n\nSegFormer processes images at 512×512 pixels internally, but our panoramas are 3328×1664. Upsampling predictions back to original resolution using bilinear interpolation preserves smooth boundaries between semantic classes while allowing us to calculate accurate pixel proportions.\n\n### Batch Processing at Scale\n\nFor the full dataset of ~40,000 images, batch processing reduces total computation time from over 100 hours to approximately **4 hours** on a single GPU. The key optimizations: multi-threaded data loading (`num_workers=4`) allows the CPU to prepare batches while the GPU processes, `pin_memory=True` speeds up CPU-to-GPU transfer, and we start with batch_size=8, reducing if memory errors occur.\n\n```python\n#| eval: false\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass GSVDataset(Dataset):\n    \"\"\"Custom dataset for batch processing street view images.\"\"\"\n    \n    def __init__(self, image_list, processor):\n        self.image_list = image_list\n        self.processor = processor\n    \n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self, idx):\n        img_info = self.image_list[idx]\n        image = cv2.imread(img_info['path'])\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        inputs = self.processor(images=image_rgb, return_tensors=\"pt\")\n        \n        return {\n            'pixel_values': inputs['pixel_values'].squeeze(),\n            'pano_id': img_info['pano_id'],\n            'point_id': img_info['point_id'],\n            'height': image.shape[0],\n            'width': image.shape[1]\n        }\n```\n\nQuality control: we track failed images (corrupted downloads, unusual dimensions) and log them for manual review. In our run, fewer than 0.5% of images failed processing.\n\n---\n\n## What the Data Reveals\n\nAfter processing all images, we can compare the visual characteristics of happiness points against baseline road samples:\n\n```python\n#| eval: false\n\nimport pandas as pd\nfrom scipy import stats\n\ndef compare_distributions(df):\n    \"\"\"\n    Statistical comparison of visual features between groups.\n    \"\"\"\n    happy = df[df['is_happy'] == 1]\n    other = df[df['is_happy'] == 0]\n    \n    features = ['sky_ratio', 'green_view_index', 'building_ratio', \n                'road_ratio', 'vehicle_ratio', 'person_ratio']\n    \n    results = []\n    for feat in features:\n        h_mean = happy[feat].mean()\n        o_mean = other[feat].mean()\n        \n        # Independent samples t-test\n        t_stat, p_value = stats.ttest_ind(\n            happy[feat].dropna(), \n            other[feat].dropna()\n        )\n        \n        results.append({\n            'feature': feat,\n            'happy_mean': h_mean,\n            'other_mean': o_mean,\n            'difference': h_mean - o_mean,\n            't_statistic': t_stat,\n            'p_value': p_value,\n            'significant': p_value < 0.05\n        })\n    \n    return pd.DataFrame(results)\n```\n\n| Feature | Happy Mean | Other Mean | Difference | p-value |\n|---------|-----------|-----------|------------|---------|\n| sky_ratio | 0.285 | 0.216 | +0.069 | 0.001** |\n| green_view_index | 0.152 | 0.149 | +0.003 | 0.847 |\n| building_ratio | 0.213 | 0.188 | +0.025 | 0.034* |\n| road_ratio | 0.196 | 0.223 | -0.027 | 0.048* |\n| vehicle_ratio | 0.023 | 0.031 | -0.008 | 0.156 |\n| person_ratio | 0.008 | 0.011 | -0.003 | 0.423 |\n\nThe results tell an interesting story. **Sky visibility is significantly higher** at happiness points—nearly 7 percentage points more than the city average. This aligns with research on the psychological benefits of open views and natural light. **Building ratio is also higher**, suggesting happiness points cluster in denser urban areas with more amenities rather than sparse residential neighborhoods. Meanwhile, **road coverage is lower**, potentially indicating more pedestrian-friendly environments.\n\nPerhaps most surprising: **green view index shows no significant difference**. Vegetation is fairly uniform across Philadelphia, or perhaps small patches of greenery don't meaningfully differentiate happy from ordinary locations. This finding warrants further investigation.\n\n---\n\n## Visualizing Segmentation Results\n\nFor quality assurance and interpretability, we generate visualizations showing the original panorama alongside its semantic segmentation:\n\n```python\n#| eval: false\n\ndef create_visualization(original_image, segmentation, feature_extractor, point_id):\n    \"\"\"\n    Create a side-by-side visualization of original image and segmentation.\n    \"\"\"\n    h, w = segmentation.shape\n    \n    # Create colored segmentation mask\n    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n    \n    for category, class_ids in feature_extractor.categories.items():\n        mask = np.isin(segmentation, class_ids)\n        color = feature_extractor.colors[category]\n        color_mask[mask] = color\n    \n    # Create overlay\n    color_mask_bgr = cv2.cvtColor(color_mask, cv2.COLOR_RGB2BGR)\n    overlay = cv2.addWeighted(original_image, 0.6, color_mask_bgr, 0.4, 0)\n    \n    # Combine: original | segmentation | overlay\n    combined = np.hstack([original_image, color_mask_bgr, overlay])\n    \n    return combined\n```\n\n### Four Environments, Four Stories\n\n#### Urban Park — High Happiness Score\n\n![Segmentation: Urban park with high greenery](../images/jJkm1xU1q2wDq3tZ6HIXKA_seg.jpg){width=100%}\n\nThis happiness point features abundant vegetation (dark and light green), significant sky visibility (blue), moderate building presence (gray), and pedestrian-friendly paths (light gray). The Green View Index exceeds 23%, and sky ratio surpasses 24%—both well above city averages. Open, green, and human-scaled: this exemplifies the \"ideal\" happy place.\n\n#### Community Garden — High Happiness Score\n\n![Segmentation: Residential street with community garden](../images/7fxU5aDxWqvtI3lmblf2RQ_seg.jpg){width=100%}\n\nA residential street with a vibrant community garden. The segmentation captures vegetation from the garden and street trees (green), row houses (gray), moderate sky visibility, and limited road surface. The presence of visible human-scale elements—plants, stoops, small buildings—creates an intimate, inviting atmosphere.\n\n#### Commercial District — Moderate Score\n\n![Segmentation: Urban commercial street](../images/gkG_zMm_kd5sezfg4OcyCw_seg.jpg){width=100%}\n\nA typical University City commercial street. Building ratio is high (~22%), sky visibility moderate, with notable vehicle presence (red). Curved building facades and street trees provide visual interest, but significant road coverage (~42%) and parked vehicles reduce the happiness score compared to pedestrian-oriented spaces.\n\n#### Highway Infrastructure — Low Score\n\n![Segmentation: Highway with high road ratio](../images/177Csfd8e6xqewWrF-dq5A_seg.jpg){width=100%}\n\nThis location exemplifies features our model associates with low happiness: dominant road coverage (~42%), multiple vehicles, minimal vegetation (only 0.2% green). Despite high sky visibility (57%), the environment feels hostile to pedestrians. Even with open sky, a space designed for vehicles rather than people scores poorly.\n\n### The Pattern in Numbers\n\n| Location | Sky | Green | Building | Road | Vehicle |\n|----------|-----|-------|----------|------|---------|\n| Urban Park | 24.3% | 23.8% | 2.0% | 25.1% | 0.0% |\n| Community Garden | 31.2% | 12.8% | 33.5% | 30.1% | 1.8% |\n| Commercial District | 22.5% | 4.1% | 21.9% | 42.6% | 3.2% |\n| Highway | 57.5% | 0.2% | 3.0% | 42.0% | 4.5% |\n\nHappiness points have more greenery, less road, and less vehicle presence—regardless of sky ratio.\n\n---\n\n## Limitations Worth Noting\n\nOur feature extraction has limitations that affect interpretation. We measure simple sky pixel proportion rather than the more sophisticated Sky View Factor (SVF) that accounts for hemispherical projection and better represents human perception of \"openness.\" Images capture a single moment—time-of-day, seasonal, and weather variations go unmeasured. Objects in the foreground may occlude important background features: a parked truck could hide a beautiful park. And SegFormer occasionally misclassifies objects, particularly at boundaries or for unusual urban elements like street art or construction zones.\n\n---\n\n## What We've Learned\n\nThis stage transformed ~40,000 street view images into a structured dataset of 6 visual features per location. Happiness points have significantly more sky visibility and higher building density, but less road coverage—suggesting pedestrian-friendly, vibrant urban environments rather than quiet residential streets or car-dominated corridors.\n\nThese visual features, combined with Census socioeconomic data, form the input for our PU Learning model.\n","srcMarkdownNoYaml":"\n\n## From Pixels to Meaning\n\nA street view image contains rich information about the urban environment, but raw pixel values tell us nothing directly useful. What we need is a way to answer questions like: How much of this scene is sky? How much is vegetation? Is this a car-dominated environment or a pedestrian-friendly one?\n\n**Semantic segmentation** transforms images into meaningful categories, labeling every pixel with what it represents—sky, tree, building, road, car, person. This transformation is the bridge between visual data and quantitative analysis.\n\n---\n\n## Why SegFormer?\n\nWe selected **SegFormer-B0** (Xie et al., 2021) after considering several alternatives. The model is pre-trained on ADE20K, a dataset containing 150 semantic classes with excellent coverage of urban scenes. The B0 variant represents the smallest in the SegFormer family, but for our purposes this is a feature rather than a limitation: processing ~40,000 images demands efficiency, and SegFormer-B0 delivers roughly 2 images per second on a mid-range GPU while using only ~4GB of VRAM.\n\n```python\n#| eval: false\n\nfrom transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\nimport torch\n\n# Load pre-trained model\nmodel_name = \"nvidia/segformer-b0-finetuned-ade-512-512\"\nprocessor = SegformerImageProcessor.from_pretrained(model_name)\nmodel = SegformerForSemanticSegmentation.from_pretrained(model_name)\n\n# Check available device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\nmodel.eval()\n```\n\nWhile newer models might achieve marginally higher accuracy on benchmarks, SegFormer's transformer-based architecture proves robust across diverse urban scenes—from tree-lined residential streets to concrete highway interchanges. For tens of thousands of images, practical considerations outweigh small accuracy differences.\n\n---\n\n## Defining Urban Categories\n\nADE20K provides 150 fine-grained classes, far more granularity than we need. For urban happiness analysis, we aggregate these into **6 meaningful categories**:\n\n```python\n#| eval: false\n\nclass UrbanFeatureExtractor:\n    \"\"\"\n    Extract urban environmental features from semantic segmentation results.\n    \n    Aggregates ADE20K's 150 classes into 6 urban-relevant categories.\n    \"\"\"\n    \n    def __init__(self):\n        # ADE20K class indices for each urban category\n        self.categories = {\n            'sky': [2],                          # sky\n            'vegetation': [4, 9, 17, 66, 72],    # tree, grass, plant, palm, flower\n            'building': [1, 25, 48, 84],         # building, house, skyscraper, booth\n            'road': [6, 11, 52],                 # road, sidewalk, path\n            'vehicle': [20, 80, 83, 102, 127],   # car, bus, truck, van, bicycle\n            'person': [12]                       # person\n        }\n        \n        # Colors for visualization (RGB)\n        self.colors = {\n            'sky': [135, 206, 235],      # Light blue\n            'vegetation': [34, 139, 34],  # Forest green\n            'building': [128, 128, 128],  # Gray\n            'road': [64, 64, 64],         # Dark gray\n            'vehicle': [255, 0, 0],       # Red\n            'person': [255, 192, 203]     # Pink\n        }\n    \n    def calculate_ratios(self, segmentation_map):\n        \"\"\"\n        Calculate the proportion of each category in the image.\n        \n        Parameters:\n        -----------\n        segmentation_map : numpy array\n            2D array of class predictions (H x W)\n        \n        Returns:\n        --------\n        dict : Category ratios (values sum to less than 1.0 due to uncategorized pixels)\n        \"\"\"\n        total_pixels = segmentation_map.size\n        ratios = {}\n        \n        for category, class_ids in self.categories.items():\n            # Count pixels belonging to this category\n            mask = np.isin(segmentation_map, class_ids)\n            pixel_count = np.sum(mask)\n            ratios[f'{category}_ratio'] = pixel_count / total_pixels\n        \n        # Green View Index is a common metric\n        ratios['green_view_index'] = ratios.pop('vegetation_ratio')\n        \n        return ratios\n```\n\nThe rationale behind these groupings reflects both practical and theoretical considerations. **Sky** remains a single class because it represents a unified visual experience—the sense of openness that comes from seeing unobstructed sky. **Vegetation** combines trees, grass, and plants because prior research on Green View Index treats all greenery similarly for psychological impact (Asgarzadeh et al., 2012). **Buildings** includes multiple structure types because we care about overall built environment density, not architectural distinctions. **Road** encompasses sidewalks and paths as collective transportation infrastructure.\n\nThe remaining pixels—furniture, signage, water, and other elements—typically constitute 5-15% of any given image.\n\n---\n\n## Processing Pipeline\n\n### Single Image Segmentation\n\n```python\n#| eval: false\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\ndef segment_image(image_path, model, processor, device):\n    \"\"\"\n    Perform semantic segmentation on a single street view image.\n    \n    Returns both the segmentation map and calculated feature ratios.\n    \"\"\"\n    # Load and prepare image\n    image = cv2.imread(str(image_path))\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    pil_image = Image.fromarray(image_rgb)\n    \n    # Process through model\n    inputs = processor(images=pil_image, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n    \n    # Upsample to original resolution\n    h, w = image.shape[:2]\n    upsampled = torch.nn.functional.interpolate(\n        logits,\n        size=(h, w),\n        mode='bilinear',\n        align_corners=False\n    )\n    \n    # Get class predictions\n    segmentation = upsampled.argmax(dim=1).squeeze().cpu().numpy()\n    \n    return segmentation, image\n```\n\nSegFormer processes images at 512×512 pixels internally, but our panoramas are 3328×1664. Upsampling predictions back to original resolution using bilinear interpolation preserves smooth boundaries between semantic classes while allowing us to calculate accurate pixel proportions.\n\n### Batch Processing at Scale\n\nFor the full dataset of ~40,000 images, batch processing reduces total computation time from over 100 hours to approximately **4 hours** on a single GPU. The key optimizations: multi-threaded data loading (`num_workers=4`) allows the CPU to prepare batches while the GPU processes, `pin_memory=True` speeds up CPU-to-GPU transfer, and we start with batch_size=8, reducing if memory errors occur.\n\n```python\n#| eval: false\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass GSVDataset(Dataset):\n    \"\"\"Custom dataset for batch processing street view images.\"\"\"\n    \n    def __init__(self, image_list, processor):\n        self.image_list = image_list\n        self.processor = processor\n    \n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self, idx):\n        img_info = self.image_list[idx]\n        image = cv2.imread(img_info['path'])\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        inputs = self.processor(images=image_rgb, return_tensors=\"pt\")\n        \n        return {\n            'pixel_values': inputs['pixel_values'].squeeze(),\n            'pano_id': img_info['pano_id'],\n            'point_id': img_info['point_id'],\n            'height': image.shape[0],\n            'width': image.shape[1]\n        }\n```\n\nQuality control: we track failed images (corrupted downloads, unusual dimensions) and log them for manual review. In our run, fewer than 0.5% of images failed processing.\n\n---\n\n## What the Data Reveals\n\nAfter processing all images, we can compare the visual characteristics of happiness points against baseline road samples:\n\n```python\n#| eval: false\n\nimport pandas as pd\nfrom scipy import stats\n\ndef compare_distributions(df):\n    \"\"\"\n    Statistical comparison of visual features between groups.\n    \"\"\"\n    happy = df[df['is_happy'] == 1]\n    other = df[df['is_happy'] == 0]\n    \n    features = ['sky_ratio', 'green_view_index', 'building_ratio', \n                'road_ratio', 'vehicle_ratio', 'person_ratio']\n    \n    results = []\n    for feat in features:\n        h_mean = happy[feat].mean()\n        o_mean = other[feat].mean()\n        \n        # Independent samples t-test\n        t_stat, p_value = stats.ttest_ind(\n            happy[feat].dropna(), \n            other[feat].dropna()\n        )\n        \n        results.append({\n            'feature': feat,\n            'happy_mean': h_mean,\n            'other_mean': o_mean,\n            'difference': h_mean - o_mean,\n            't_statistic': t_stat,\n            'p_value': p_value,\n            'significant': p_value < 0.05\n        })\n    \n    return pd.DataFrame(results)\n```\n\n| Feature | Happy Mean | Other Mean | Difference | p-value |\n|---------|-----------|-----------|------------|---------|\n| sky_ratio | 0.285 | 0.216 | +0.069 | 0.001** |\n| green_view_index | 0.152 | 0.149 | +0.003 | 0.847 |\n| building_ratio | 0.213 | 0.188 | +0.025 | 0.034* |\n| road_ratio | 0.196 | 0.223 | -0.027 | 0.048* |\n| vehicle_ratio | 0.023 | 0.031 | -0.008 | 0.156 |\n| person_ratio | 0.008 | 0.011 | -0.003 | 0.423 |\n\nThe results tell an interesting story. **Sky visibility is significantly higher** at happiness points—nearly 7 percentage points more than the city average. This aligns with research on the psychological benefits of open views and natural light. **Building ratio is also higher**, suggesting happiness points cluster in denser urban areas with more amenities rather than sparse residential neighborhoods. Meanwhile, **road coverage is lower**, potentially indicating more pedestrian-friendly environments.\n\nPerhaps most surprising: **green view index shows no significant difference**. Vegetation is fairly uniform across Philadelphia, or perhaps small patches of greenery don't meaningfully differentiate happy from ordinary locations. This finding warrants further investigation.\n\n---\n\n## Visualizing Segmentation Results\n\nFor quality assurance and interpretability, we generate visualizations showing the original panorama alongside its semantic segmentation:\n\n```python\n#| eval: false\n\ndef create_visualization(original_image, segmentation, feature_extractor, point_id):\n    \"\"\"\n    Create a side-by-side visualization of original image and segmentation.\n    \"\"\"\n    h, w = segmentation.shape\n    \n    # Create colored segmentation mask\n    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n    \n    for category, class_ids in feature_extractor.categories.items():\n        mask = np.isin(segmentation, class_ids)\n        color = feature_extractor.colors[category]\n        color_mask[mask] = color\n    \n    # Create overlay\n    color_mask_bgr = cv2.cvtColor(color_mask, cv2.COLOR_RGB2BGR)\n    overlay = cv2.addWeighted(original_image, 0.6, color_mask_bgr, 0.4, 0)\n    \n    # Combine: original | segmentation | overlay\n    combined = np.hstack([original_image, color_mask_bgr, overlay])\n    \n    return combined\n```\n\n### Four Environments, Four Stories\n\n#### Urban Park — High Happiness Score\n\n![Segmentation: Urban park with high greenery](../images/jJkm1xU1q2wDq3tZ6HIXKA_seg.jpg){width=100%}\n\nThis happiness point features abundant vegetation (dark and light green), significant sky visibility (blue), moderate building presence (gray), and pedestrian-friendly paths (light gray). The Green View Index exceeds 23%, and sky ratio surpasses 24%—both well above city averages. Open, green, and human-scaled: this exemplifies the \"ideal\" happy place.\n\n#### Community Garden — High Happiness Score\n\n![Segmentation: Residential street with community garden](../images/7fxU5aDxWqvtI3lmblf2RQ_seg.jpg){width=100%}\n\nA residential street with a vibrant community garden. The segmentation captures vegetation from the garden and street trees (green), row houses (gray), moderate sky visibility, and limited road surface. The presence of visible human-scale elements—plants, stoops, small buildings—creates an intimate, inviting atmosphere.\n\n#### Commercial District — Moderate Score\n\n![Segmentation: Urban commercial street](../images/gkG_zMm_kd5sezfg4OcyCw_seg.jpg){width=100%}\n\nA typical University City commercial street. Building ratio is high (~22%), sky visibility moderate, with notable vehicle presence (red). Curved building facades and street trees provide visual interest, but significant road coverage (~42%) and parked vehicles reduce the happiness score compared to pedestrian-oriented spaces.\n\n#### Highway Infrastructure — Low Score\n\n![Segmentation: Highway with high road ratio](../images/177Csfd8e6xqewWrF-dq5A_seg.jpg){width=100%}\n\nThis location exemplifies features our model associates with low happiness: dominant road coverage (~42%), multiple vehicles, minimal vegetation (only 0.2% green). Despite high sky visibility (57%), the environment feels hostile to pedestrians. Even with open sky, a space designed for vehicles rather than people scores poorly.\n\n### The Pattern in Numbers\n\n| Location | Sky | Green | Building | Road | Vehicle |\n|----------|-----|-------|----------|------|---------|\n| Urban Park | 24.3% | 23.8% | 2.0% | 25.1% | 0.0% |\n| Community Garden | 31.2% | 12.8% | 33.5% | 30.1% | 1.8% |\n| Commercial District | 22.5% | 4.1% | 21.9% | 42.6% | 3.2% |\n| Highway | 57.5% | 0.2% | 3.0% | 42.0% | 4.5% |\n\nHappiness points have more greenery, less road, and less vehicle presence—regardless of sky ratio.\n\n---\n\n## Limitations Worth Noting\n\nOur feature extraction has limitations that affect interpretation. We measure simple sky pixel proportion rather than the more sophisticated Sky View Factor (SVF) that accounts for hemispherical projection and better represents human perception of \"openness.\" Images capture a single moment—time-of-day, seasonal, and weather variations go unmeasured. Objects in the foreground may occlude important background features: a parked truck could hide a beautiful park. And SegFormer occasionally misclassifies objects, particularly at boundaries or for unusual urban elements like street art or construction zones.\n\n---\n\n## What We've Learned\n\nThis stage transformed ~40,000 street view images into a structured dataset of 6 visual features per location. Happiness points have significantly more sky visibility and higher building density, but less road coverage—suggesting pedestrian-friendly, vibrant urban environments rather than quiet residential streets or car-dominated corridors.\n\nThese visual features, combined with Census socioeconomic data, form the input for our PU Learning model.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"2-semantic-segmentation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.24","theme":"cosmo","toc-location":"right","toc-title":"On this page","code-summary":"Show code","page-layout":"full","grid":{"sidebar-width":"250px","body-width":"1200px","margin-width":"200px"},"title":"Semantic Segmentation","subtitle":"Extracting Urban Visual Features from Street View Imagery"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}