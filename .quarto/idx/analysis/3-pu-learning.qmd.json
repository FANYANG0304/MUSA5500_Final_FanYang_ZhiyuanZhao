{"title":"PU Learning","markdown":{"yaml":{"title":"PU Learning","subtitle":"Modeling Happiness with Positive-Unlabeled Learning"},"headingText":"The Problem with \"Negative\" Labels","containsRefs":false,"markdown":"\n\n\nTraditional binary classification requires both positive and negative labeled examples. Our dataset contains 28 happiness points marked by survey respondents and ~40,000 road sampling points with unknown happiness status. The tempting approach would be to treat all unlabeled points as \"not happy\"—but this fundamentally misrepresents the data.\n\nThe critical insight: **unlabeled does not mean negative**. Many road sampling points might also be happy locations—they simply weren't identified in the survey. A beautiful park bench, a cozy café corner, a tree-lined street with afternoon sun—any of these might bring joy to someone, but if no survey respondent happened to mention them, they remain unlabeled. Training a standard classifier with all unlabeled points as negatives would systematically bias the model against places that are happy but unrecognized.\n\n**Positive-Unlabeled (PU) Learning** addresses this by identifying \"reliable negatives\" from the unlabeled set—points that are genuinely unlikely to be happy places—and training classifiers using only these carefully selected negatives alongside the labeled positives.\n\n```python\n#| eval: false\n\nfrom scipy.spatial.distance import cdist\nimport numpy as np\n\nclass PULearningModel:\n    \"\"\"\n    Positive-Unlabeled Learning classifier for happiness prediction.\n    \n    Uses distance-based reliable negative identification followed by\n    standard classification with class balancing.\n    \"\"\"\n    \n    def __init__(self, reliable_neg_ratio=0.30):\n        \"\"\"\n        Parameters:\n        -----------\n        reliable_neg_ratio : float\n            Proportion of unlabeled samples to use as reliable negatives.\n            Higher values increase training set size but risk label noise.\n        \"\"\"\n        self.reliable_neg_ratio = reliable_neg_ratio\n        self.scaler = StandardScaler()\n```\n\n---\n\n## Building the Feature Vector\n\nWe construct a feature vector combining visual and socioeconomic characteristics—what a person sees when standing at a location, and the broader neighborhood context that shapes how places \"feel\":\n\n```python\n#| eval: false\n\n# Visual features from semantic segmentation\nvisual_features = [\n    'sky_ratio',           # Sky visibility\n    'green_view_index',    # Vegetation coverage\n    'building_ratio',      # Built environment density\n    'road_ratio',          # Road/sidewalk coverage\n    'vehicle_ratio',       # Vehicle presence\n    'person_ratio'         # Pedestrian presence\n]\n\n# Socioeconomic features from Census\ncensus_features = [\n    'median_income',       # Economic status\n    'poverty_rate',        # Deprivation indicator\n    'pct_college',         # Education level\n    'pct_white',           # Racial composition\n    'median_age',          # Age demographics\n    'pct_owner_occupied',  # Housing tenure\n    'unemployment_rate'    # Employment status\n]\n\nall_features = visual_features + census_features\n```\n\nVisual features directly measure what a person sees. Prior research has shown that these characteristics correlate with perceived safety and pleasantness. Socioeconomic features capture neighborhood context that may not be directly visible—a clean street in a high-income area may feel different from an identical street in a high-poverty area due to ambient factors like noise, social activity, and maintenance levels. We intentionally excluded highly correlated features (using `pct_college` rather than separate counts for each degree level) to avoid multicollinearity issues in logistic regression.\n\n---\n\n## Finding Reliable Negatives\n\nOur approach identifies reliable negatives based on feature-space distance from positive examples. The intuition: points far from the positive centroid in feature space are unlikely to be latent positives.\n\n```python\n#| eval: false\n\ndef identify_reliable_negatives(self, X_positive, X_unlabeled):\n    \"\"\"\n    Identify reliable negative samples using distance to positive centroid.\n    \n    Intuition: Points far from the positive centroid in feature space\n    are unlikely to be latent positives.\n    \"\"\"\n    # Calculate centroid of positive samples\n    positive_centroid = X_positive.mean(axis=0)\n    \n    # Calculate Euclidean distance from each unlabeled point to centroid\n    distances = cdist(X_unlabeled, [positive_centroid], metric='euclidean').flatten()\n    \n    # Select most distant points as reliable negatives\n    n_reliable = int(len(X_unlabeled) * self.reliable_neg_ratio)\n    reliable_neg_indices = np.argsort(distances)[-n_reliable:]\n    \n    # Also identify \"suspect positives\" (very close to positive centroid)\n    n_suspect = int(len(X_unlabeled) * 0.05)\n    suspect_pos_indices = np.argsort(distances)[:n_suspect]\n    \n    return reliable_neg_indices, suspect_pos_indices, distances\n```\n\nWe chose **30% as the reliable negative ratio** after considering several factors. Too few negatives limits training data; too many risks including latent positives. Urban planning literature suggests roughly 60-70% of city locations are \"neutral\"—neither particularly happy nor unhappy. Cross-validation performance plateaued around 25-35%.\n\nThe **5% suspect positive** identification serves a dual purpose: these points are excluded from the reliable negative set as a safety margin, and they can be examined post-hoc to discover potentially overlooked happy locations.\n\n```python\n#| eval: false\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef visualize_sample_distribution(X_pos, X_unl, reliable_idx, suspect_idx):\n    \"\"\"\n    2D PCA visualization of sample distribution.\n    \"\"\"\n    # Combine and project to 2D\n    X_all = np.vstack([X_pos, X_unl])\n    pca = PCA(n_components=2)\n    X_2d = pca.fit_transform(X_all)\n    \n    n_pos = len(X_pos)\n    \n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    # Plot each category\n    ax.scatter(X_2d[n_pos:][reliable_idx, 0], X_2d[n_pos:][reliable_idx, 1],\n               c='blue', alpha=0.3, s=10, label='Reliable Negative')\n    ax.scatter(X_2d[n_pos:][suspect_idx, 0], X_2d[n_pos:][suspect_idx, 1],\n               c='orange', alpha=0.5, s=20, label='Suspect Positive')\n    ax.scatter(X_2d[:n_pos, 0], X_2d[:n_pos, 1],\n               c='red', s=100, marker='*', label='Labeled Positive')\n    \n    ax.legend()\n    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n    ax.set_title('PCA Projection of Sample Distribution')\n    \n    return fig\n```\n\n---\n\n## Training the Models\n\nWe train two complementary classifiers. **Logistic Regression** provides interpretable coefficients showing the direction and magnitude of each feature's effect. **Random Forest** can capture non-linear relationships and interactions that logistic regression misses.\n\n```python\n#| eval: false\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\ndef train_models(self, X_positive, X_unlabeled):\n    \"\"\"\n    Train PU Learning classifiers.\n    \"\"\"\n    # Scale features\n    X_all = np.vstack([X_positive, X_unlabeled])\n    self.scaler.fit(X_all)\n    \n    X_pos_scaled = self.scaler.transform(X_positive)\n    X_unl_scaled = self.scaler.transform(X_unlabeled)\n    \n    # Identify reliable negatives\n    reliable_idx, suspect_idx, distances = self.identify_reliable_negatives(\n        X_pos_scaled, X_unl_scaled\n    )\n    \n    X_reliable_neg = X_unl_scaled[reliable_idx]\n    \n    # Prepare training data\n    X_train = np.vstack([X_pos_scaled, X_reliable_neg])\n    y_train = np.array([1] * len(X_pos_scaled) + [0] * len(X_reliable_neg))\n    \n    # Train models with class balancing\n    self.lr_model = LogisticRegression(\n        class_weight='balanced', \n        max_iter=1000,\n        random_state=42\n    )\n    self.lr_model.fit(X_train, y_train)\n    \n    self.rf_model = RandomForestClassifier(\n        n_estimators=200,\n        max_depth=10,\n        class_weight='balanced',\n        random_state=42\n    )\n    self.rf_model.fit(X_train, y_train)\n    \n    return X_train, y_train\n```\n\n**Class weighting** (`class_weight='balanced'`) is essential. Even after PU Learning, we have a 28:~9,000 class ratio. Without weighting, the model would achieve high accuracy by simply predicting all points as negative.\n\n**Cross-validation strategy**: We use 5-fold stratified cross-validation, ensuring each fold contains proportional representation of positive samples (about 5-6 per fold). This is the minimum viable setup given our small positive sample size.\n\n---\n\n## Evaluating Performance\n\n```python\n#| eval: false\n\nfrom sklearn.metrics import roc_curve, auc\n\ndef plot_roc_curves(self, X_train, y_train):\n    \"\"\"\n    Generate ROC curves with 5-fold cross-validation.\n    \"\"\"\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    fig, ax = plt.subplots(figsize=(8, 8))\n    mean_fpr = np.linspace(0, 1, 100)\n    \n    lr_tprs = []\n    rf_tprs = []\n    \n    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n        X_tr, y_tr = X_train[train_idx], y_train[train_idx]\n        X_val, y_val = X_train[val_idx], y_train[val_idx]\n        \n        # Logistic Regression\n        lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n        lr.fit(X_tr, y_tr)\n        lr_prob = lr.predict_proba(X_val)[:, 1]\n        lr_fpr, lr_tpr, _ = roc_curve(y_val, lr_prob)\n        lr_tprs.append(np.interp(mean_fpr, lr_fpr, lr_tpr))\n        \n        # Random Forest\n        rf = RandomForestClassifier(n_estimators=200, max_depth=10, \n                                    class_weight='balanced', random_state=42)\n        rf.fit(X_tr, y_tr)\n        rf_prob = rf.predict_proba(X_val)[:, 1]\n        rf_fpr, rf_tpr, _ = roc_curve(y_val, rf_prob)\n        rf_tprs.append(np.interp(mean_fpr, rf_fpr, rf_tpr))\n    \n    # Plot mean curves\n    ax.plot(mean_fpr, np.mean(lr_tprs, axis=0), \n            label=f'Logistic Regression (AUC = {np.mean([auc(mean_fpr, t) for t in lr_tprs]):.3f})')\n    ax.plot(mean_fpr, np.mean(rf_tprs, axis=0),\n            label=f'Random Forest (AUC = {np.mean([auc(mean_fpr, t) for t in rf_tprs]):.3f})')\n    ax.plot([0, 1], [0, 1], '--', color='gray', label='Random')\n    \n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.legend()\n    \n    return fig\n```\n\n![ROC Curves from 5-Fold Cross-Validation](../images/roc_curve.png){width=80%}\n\n| Model | AUC | Std Dev |\n|-------|-----|---------|\n| Logistic Regression | 0.968 | ±0.013 |\n| Random Forest | 0.939 | ±0.045 |\n\nBoth models achieve strong discriminative performance, with **Logistic Regression slightly outperforming Random Forest**. An AUC of 0.968 means that if we randomly select one positive and one negative sample, the model correctly ranks them 96.8% of the time. Happiness points are indeed distinguishable from reliable negatives in feature space.\n\nWhy does the simpler model win? With only 28 positive samples, Random Forest's flexibility becomes a liability—it can overfit to idiosyncratic patterns in the small positive set. Logistic regression's stronger inductive bias (linear decision boundary) provides regularization that helps generalization. The variance comparison confirms this: logistic regression shows lower variance across folds (±0.013 vs ±0.045), indicating more stable performance.\n\n---\n\n## What the Coefficients Tell Us\n\n```python\n#| eval: false\n\ndef get_feature_importance(self, feature_names):\n    \"\"\"\n    Extract and format feature importance from both models.\n    \"\"\"\n    importance = pd.DataFrame({\n        'feature': feature_names,\n        'lr_coefficient': self.lr_model.coef_[0],\n        'rf_importance': self.rf_model.feature_importances_\n    })\n    \n    importance['direction'] = importance['lr_coefficient'].apply(\n        lambda x: 'Positive (+)' if x > 0 else 'Negative (-)'\n    )\n    importance['lr_abs'] = importance['lr_coefficient'].abs()\n    \n    return importance.sort_values('lr_abs', ascending=False)\n```\n\n| Feature | LR Coefficient | Direction | RF Importance |\n|---------|---------------|-----------|---------------|\n| pct_owner_occupied | -2.10 | Negative | 0.089 |\n| poverty_rate | -1.28 | Negative | 0.076 |\n| road_ratio | -1.23 | Negative | 0.082 |\n| building_ratio | +1.00 | Positive | 0.095 |\n| pct_college | +0.70 | Positive | 0.088 |\n| sky_ratio | +0.58 | Positive | 0.091 |\n| vehicle_ratio | -0.29 | Negative | 0.078 |\n| person_ratio | -0.24 | Negative | 0.065 |\n| median_income | +0.18 | Positive | 0.112 |\n| green_view_index | +0.06 | Positive | 0.087 |\n| median_age | -0.04 | Negative | 0.071 |\n| unemployment_rate | -0.02 | Negative | 0.066 |\n\nThe **strongest negative predictor is owner-occupancy rate (-2.10)**—a counterintuitive finding. Areas with high homeownership are less likely to contain happiness points. High-ownership areas tend to be quiet residential suburbs with few destinations; happiness points may cluster in mixed-use, walkable areas that have more renters. This reflects a distinction between \"good for living\" and \"feels happy to visit.\"\n\n**Road ratio as negative predictor (-1.23)** aligns with urban design principles: car-centric environments are less pleasant for pedestrians. **Building ratio as positive predictor (+1.00)** suggests dense areas have more amenities, activities, and social opportunities. **Sky visibility (+0.58)** supports the psychological value of openness in urban environments, even when controlling for building density.\n\n---\n\n## Scoring the City\n\n```python\n#| eval: false\n\ndef predict_happiness(self, df, feature_cols):\n    \"\"\"\n    Generate happiness predictions for all points.\n    \"\"\"\n    X = df[feature_cols].values\n    X_scaled = self.scaler.transform(X)\n    \n    # Get probability scores\n    raw_probs = self.lr_model.predict_proba(X_scaled)[:, 1]\n    \n    # Normalize to 0-1 scale\n    normalized = (raw_probs - raw_probs.min()) / (raw_probs.max() - raw_probs.min())\n    \n    # Ensure labeled positives get score 1.0\n    if 'is_happy' in df.columns:\n        normalized[df['is_happy'] == 1] = 1.0\n    \n    df['happiness_score'] = normalized\n    \n    return df\n```\n\nAfter scoring all ~40,000 points:\n\n| Score Range | Count | Percentage |\n|-------------|-------|------------|\n| 0.0 - 0.2 | ~9,000 | 30% |\n| 0.2 - 0.4 | ~8,000 | 27% |\n| 0.4 - 0.6 | ~7,000 | 23% |\n| 0.6 - 0.8 | ~4,000 | 13% |\n| 0.8 - 1.0 | ~2,000 | 7% |\n\nThe distribution is **right-skewed**, with most points receiving low happiness scores. This is expected—if happiness points were common, they wouldn't be special. The ~2,000 points (7%) scoring above 0.8 represent locations sharing characteristics with labeled happiness points. These could be true latent positives—happy places not identified in the survey—or false positives that statistically resemble happy places but don't evoke actual happiness. Manual validation of a random sample could help distinguish these cases.\n\n---\n\n## What We've Built\n\nThe PU Learning approach successfully identifies features that distinguish happiness points. Distance-based reliable negative identification combined with class-balanced logistic regression achieves AUC = 0.968. The key positive factors are building density, college education rate, and sky visibility. The key negative factors are owner-occupancy rate, poverty rate, and road coverage. We've generated happiness scores for all ~40,000 sampling points, enabling city-wide mapping of predicted happiness.\n\nThe next section presents the full results and discusses what these patterns mean for understanding—and perhaps improving—urban environments.\n","srcMarkdownNoYaml":"\n\n## The Problem with \"Negative\" Labels\n\nTraditional binary classification requires both positive and negative labeled examples. Our dataset contains 28 happiness points marked by survey respondents and ~40,000 road sampling points with unknown happiness status. The tempting approach would be to treat all unlabeled points as \"not happy\"—but this fundamentally misrepresents the data.\n\nThe critical insight: **unlabeled does not mean negative**. Many road sampling points might also be happy locations—they simply weren't identified in the survey. A beautiful park bench, a cozy café corner, a tree-lined street with afternoon sun—any of these might bring joy to someone, but if no survey respondent happened to mention them, they remain unlabeled. Training a standard classifier with all unlabeled points as negatives would systematically bias the model against places that are happy but unrecognized.\n\n**Positive-Unlabeled (PU) Learning** addresses this by identifying \"reliable negatives\" from the unlabeled set—points that are genuinely unlikely to be happy places—and training classifiers using only these carefully selected negatives alongside the labeled positives.\n\n```python\n#| eval: false\n\nfrom scipy.spatial.distance import cdist\nimport numpy as np\n\nclass PULearningModel:\n    \"\"\"\n    Positive-Unlabeled Learning classifier for happiness prediction.\n    \n    Uses distance-based reliable negative identification followed by\n    standard classification with class balancing.\n    \"\"\"\n    \n    def __init__(self, reliable_neg_ratio=0.30):\n        \"\"\"\n        Parameters:\n        -----------\n        reliable_neg_ratio : float\n            Proportion of unlabeled samples to use as reliable negatives.\n            Higher values increase training set size but risk label noise.\n        \"\"\"\n        self.reliable_neg_ratio = reliable_neg_ratio\n        self.scaler = StandardScaler()\n```\n\n---\n\n## Building the Feature Vector\n\nWe construct a feature vector combining visual and socioeconomic characteristics—what a person sees when standing at a location, and the broader neighborhood context that shapes how places \"feel\":\n\n```python\n#| eval: false\n\n# Visual features from semantic segmentation\nvisual_features = [\n    'sky_ratio',           # Sky visibility\n    'green_view_index',    # Vegetation coverage\n    'building_ratio',      # Built environment density\n    'road_ratio',          # Road/sidewalk coverage\n    'vehicle_ratio',       # Vehicle presence\n    'person_ratio'         # Pedestrian presence\n]\n\n# Socioeconomic features from Census\ncensus_features = [\n    'median_income',       # Economic status\n    'poverty_rate',        # Deprivation indicator\n    'pct_college',         # Education level\n    'pct_white',           # Racial composition\n    'median_age',          # Age demographics\n    'pct_owner_occupied',  # Housing tenure\n    'unemployment_rate'    # Employment status\n]\n\nall_features = visual_features + census_features\n```\n\nVisual features directly measure what a person sees. Prior research has shown that these characteristics correlate with perceived safety and pleasantness. Socioeconomic features capture neighborhood context that may not be directly visible—a clean street in a high-income area may feel different from an identical street in a high-poverty area due to ambient factors like noise, social activity, and maintenance levels. We intentionally excluded highly correlated features (using `pct_college` rather than separate counts for each degree level) to avoid multicollinearity issues in logistic regression.\n\n---\n\n## Finding Reliable Negatives\n\nOur approach identifies reliable negatives based on feature-space distance from positive examples. The intuition: points far from the positive centroid in feature space are unlikely to be latent positives.\n\n```python\n#| eval: false\n\ndef identify_reliable_negatives(self, X_positive, X_unlabeled):\n    \"\"\"\n    Identify reliable negative samples using distance to positive centroid.\n    \n    Intuition: Points far from the positive centroid in feature space\n    are unlikely to be latent positives.\n    \"\"\"\n    # Calculate centroid of positive samples\n    positive_centroid = X_positive.mean(axis=0)\n    \n    # Calculate Euclidean distance from each unlabeled point to centroid\n    distances = cdist(X_unlabeled, [positive_centroid], metric='euclidean').flatten()\n    \n    # Select most distant points as reliable negatives\n    n_reliable = int(len(X_unlabeled) * self.reliable_neg_ratio)\n    reliable_neg_indices = np.argsort(distances)[-n_reliable:]\n    \n    # Also identify \"suspect positives\" (very close to positive centroid)\n    n_suspect = int(len(X_unlabeled) * 0.05)\n    suspect_pos_indices = np.argsort(distances)[:n_suspect]\n    \n    return reliable_neg_indices, suspect_pos_indices, distances\n```\n\nWe chose **30% as the reliable negative ratio** after considering several factors. Too few negatives limits training data; too many risks including latent positives. Urban planning literature suggests roughly 60-70% of city locations are \"neutral\"—neither particularly happy nor unhappy. Cross-validation performance plateaued around 25-35%.\n\nThe **5% suspect positive** identification serves a dual purpose: these points are excluded from the reliable negative set as a safety margin, and they can be examined post-hoc to discover potentially overlooked happy locations.\n\n```python\n#| eval: false\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef visualize_sample_distribution(X_pos, X_unl, reliable_idx, suspect_idx):\n    \"\"\"\n    2D PCA visualization of sample distribution.\n    \"\"\"\n    # Combine and project to 2D\n    X_all = np.vstack([X_pos, X_unl])\n    pca = PCA(n_components=2)\n    X_2d = pca.fit_transform(X_all)\n    \n    n_pos = len(X_pos)\n    \n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    # Plot each category\n    ax.scatter(X_2d[n_pos:][reliable_idx, 0], X_2d[n_pos:][reliable_idx, 1],\n               c='blue', alpha=0.3, s=10, label='Reliable Negative')\n    ax.scatter(X_2d[n_pos:][suspect_idx, 0], X_2d[n_pos:][suspect_idx, 1],\n               c='orange', alpha=0.5, s=20, label='Suspect Positive')\n    ax.scatter(X_2d[:n_pos, 0], X_2d[:n_pos, 1],\n               c='red', s=100, marker='*', label='Labeled Positive')\n    \n    ax.legend()\n    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n    ax.set_title('PCA Projection of Sample Distribution')\n    \n    return fig\n```\n\n---\n\n## Training the Models\n\nWe train two complementary classifiers. **Logistic Regression** provides interpretable coefficients showing the direction and magnitude of each feature's effect. **Random Forest** can capture non-linear relationships and interactions that logistic regression misses.\n\n```python\n#| eval: false\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\ndef train_models(self, X_positive, X_unlabeled):\n    \"\"\"\n    Train PU Learning classifiers.\n    \"\"\"\n    # Scale features\n    X_all = np.vstack([X_positive, X_unlabeled])\n    self.scaler.fit(X_all)\n    \n    X_pos_scaled = self.scaler.transform(X_positive)\n    X_unl_scaled = self.scaler.transform(X_unlabeled)\n    \n    # Identify reliable negatives\n    reliable_idx, suspect_idx, distances = self.identify_reliable_negatives(\n        X_pos_scaled, X_unl_scaled\n    )\n    \n    X_reliable_neg = X_unl_scaled[reliable_idx]\n    \n    # Prepare training data\n    X_train = np.vstack([X_pos_scaled, X_reliable_neg])\n    y_train = np.array([1] * len(X_pos_scaled) + [0] * len(X_reliable_neg))\n    \n    # Train models with class balancing\n    self.lr_model = LogisticRegression(\n        class_weight='balanced', \n        max_iter=1000,\n        random_state=42\n    )\n    self.lr_model.fit(X_train, y_train)\n    \n    self.rf_model = RandomForestClassifier(\n        n_estimators=200,\n        max_depth=10,\n        class_weight='balanced',\n        random_state=42\n    )\n    self.rf_model.fit(X_train, y_train)\n    \n    return X_train, y_train\n```\n\n**Class weighting** (`class_weight='balanced'`) is essential. Even after PU Learning, we have a 28:~9,000 class ratio. Without weighting, the model would achieve high accuracy by simply predicting all points as negative.\n\n**Cross-validation strategy**: We use 5-fold stratified cross-validation, ensuring each fold contains proportional representation of positive samples (about 5-6 per fold). This is the minimum viable setup given our small positive sample size.\n\n---\n\n## Evaluating Performance\n\n```python\n#| eval: false\n\nfrom sklearn.metrics import roc_curve, auc\n\ndef plot_roc_curves(self, X_train, y_train):\n    \"\"\"\n    Generate ROC curves with 5-fold cross-validation.\n    \"\"\"\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    fig, ax = plt.subplots(figsize=(8, 8))\n    mean_fpr = np.linspace(0, 1, 100)\n    \n    lr_tprs = []\n    rf_tprs = []\n    \n    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n        X_tr, y_tr = X_train[train_idx], y_train[train_idx]\n        X_val, y_val = X_train[val_idx], y_train[val_idx]\n        \n        # Logistic Regression\n        lr = LogisticRegression(class_weight='balanced', max_iter=1000)\n        lr.fit(X_tr, y_tr)\n        lr_prob = lr.predict_proba(X_val)[:, 1]\n        lr_fpr, lr_tpr, _ = roc_curve(y_val, lr_prob)\n        lr_tprs.append(np.interp(mean_fpr, lr_fpr, lr_tpr))\n        \n        # Random Forest\n        rf = RandomForestClassifier(n_estimators=200, max_depth=10, \n                                    class_weight='balanced', random_state=42)\n        rf.fit(X_tr, y_tr)\n        rf_prob = rf.predict_proba(X_val)[:, 1]\n        rf_fpr, rf_tpr, _ = roc_curve(y_val, rf_prob)\n        rf_tprs.append(np.interp(mean_fpr, rf_fpr, rf_tpr))\n    \n    # Plot mean curves\n    ax.plot(mean_fpr, np.mean(lr_tprs, axis=0), \n            label=f'Logistic Regression (AUC = {np.mean([auc(mean_fpr, t) for t in lr_tprs]):.3f})')\n    ax.plot(mean_fpr, np.mean(rf_tprs, axis=0),\n            label=f'Random Forest (AUC = {np.mean([auc(mean_fpr, t) for t in rf_tprs]):.3f})')\n    ax.plot([0, 1], [0, 1], '--', color='gray', label='Random')\n    \n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.legend()\n    \n    return fig\n```\n\n![ROC Curves from 5-Fold Cross-Validation](../images/roc_curve.png){width=80%}\n\n| Model | AUC | Std Dev |\n|-------|-----|---------|\n| Logistic Regression | 0.968 | ±0.013 |\n| Random Forest | 0.939 | ±0.045 |\n\nBoth models achieve strong discriminative performance, with **Logistic Regression slightly outperforming Random Forest**. An AUC of 0.968 means that if we randomly select one positive and one negative sample, the model correctly ranks them 96.8% of the time. Happiness points are indeed distinguishable from reliable negatives in feature space.\n\nWhy does the simpler model win? With only 28 positive samples, Random Forest's flexibility becomes a liability—it can overfit to idiosyncratic patterns in the small positive set. Logistic regression's stronger inductive bias (linear decision boundary) provides regularization that helps generalization. The variance comparison confirms this: logistic regression shows lower variance across folds (±0.013 vs ±0.045), indicating more stable performance.\n\n---\n\n## What the Coefficients Tell Us\n\n```python\n#| eval: false\n\ndef get_feature_importance(self, feature_names):\n    \"\"\"\n    Extract and format feature importance from both models.\n    \"\"\"\n    importance = pd.DataFrame({\n        'feature': feature_names,\n        'lr_coefficient': self.lr_model.coef_[0],\n        'rf_importance': self.rf_model.feature_importances_\n    })\n    \n    importance['direction'] = importance['lr_coefficient'].apply(\n        lambda x: 'Positive (+)' if x > 0 else 'Negative (-)'\n    )\n    importance['lr_abs'] = importance['lr_coefficient'].abs()\n    \n    return importance.sort_values('lr_abs', ascending=False)\n```\n\n| Feature | LR Coefficient | Direction | RF Importance |\n|---------|---------------|-----------|---------------|\n| pct_owner_occupied | -2.10 | Negative | 0.089 |\n| poverty_rate | -1.28 | Negative | 0.076 |\n| road_ratio | -1.23 | Negative | 0.082 |\n| building_ratio | +1.00 | Positive | 0.095 |\n| pct_college | +0.70 | Positive | 0.088 |\n| sky_ratio | +0.58 | Positive | 0.091 |\n| vehicle_ratio | -0.29 | Negative | 0.078 |\n| person_ratio | -0.24 | Negative | 0.065 |\n| median_income | +0.18 | Positive | 0.112 |\n| green_view_index | +0.06 | Positive | 0.087 |\n| median_age | -0.04 | Negative | 0.071 |\n| unemployment_rate | -0.02 | Negative | 0.066 |\n\nThe **strongest negative predictor is owner-occupancy rate (-2.10)**—a counterintuitive finding. Areas with high homeownership are less likely to contain happiness points. High-ownership areas tend to be quiet residential suburbs with few destinations; happiness points may cluster in mixed-use, walkable areas that have more renters. This reflects a distinction between \"good for living\" and \"feels happy to visit.\"\n\n**Road ratio as negative predictor (-1.23)** aligns with urban design principles: car-centric environments are less pleasant for pedestrians. **Building ratio as positive predictor (+1.00)** suggests dense areas have more amenities, activities, and social opportunities. **Sky visibility (+0.58)** supports the psychological value of openness in urban environments, even when controlling for building density.\n\n---\n\n## Scoring the City\n\n```python\n#| eval: false\n\ndef predict_happiness(self, df, feature_cols):\n    \"\"\"\n    Generate happiness predictions for all points.\n    \"\"\"\n    X = df[feature_cols].values\n    X_scaled = self.scaler.transform(X)\n    \n    # Get probability scores\n    raw_probs = self.lr_model.predict_proba(X_scaled)[:, 1]\n    \n    # Normalize to 0-1 scale\n    normalized = (raw_probs - raw_probs.min()) / (raw_probs.max() - raw_probs.min())\n    \n    # Ensure labeled positives get score 1.0\n    if 'is_happy' in df.columns:\n        normalized[df['is_happy'] == 1] = 1.0\n    \n    df['happiness_score'] = normalized\n    \n    return df\n```\n\nAfter scoring all ~40,000 points:\n\n| Score Range | Count | Percentage |\n|-------------|-------|------------|\n| 0.0 - 0.2 | ~9,000 | 30% |\n| 0.2 - 0.4 | ~8,000 | 27% |\n| 0.4 - 0.6 | ~7,000 | 23% |\n| 0.6 - 0.8 | ~4,000 | 13% |\n| 0.8 - 1.0 | ~2,000 | 7% |\n\nThe distribution is **right-skewed**, with most points receiving low happiness scores. This is expected—if happiness points were common, they wouldn't be special. The ~2,000 points (7%) scoring above 0.8 represent locations sharing characteristics with labeled happiness points. These could be true latent positives—happy places not identified in the survey—or false positives that statistically resemble happy places but don't evoke actual happiness. Manual validation of a random sample could help distinguish these cases.\n\n---\n\n## What We've Built\n\nThe PU Learning approach successfully identifies features that distinguish happiness points. Distance-based reliable negative identification combined with class-balanced logistic regression achieves AUC = 0.968. The key positive factors are building density, college education rate, and sky visibility. The key negative factors are owner-occupancy rate, poverty rate, and road coverage. We've generated happiness scores for all ~40,000 sampling points, enabling city-wide mapping of predicted happiness.\n\nThe next section presents the full results and discusses what these patterns mean for understanding—and perhaps improving—urban environments.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"3-pu-learning.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.24","theme":"cosmo","toc-location":"right","toc-title":"On this page","code-summary":"Show code","page-layout":"full","grid":{"sidebar-width":"250px","body-width":"1200px","margin-width":"200px"},"title":"PU Learning","subtitle":"Modeling Happiness with Positive-Unlabeled Learning"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}